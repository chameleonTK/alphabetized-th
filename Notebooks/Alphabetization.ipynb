{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "276bb3d3-e2df-4025-9eb6-bf6a714b941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94442ab2-e2d5-4165-b896-df5e0845fcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Datasets.CharTokenizer import CharTokenizer\n",
    "from Datasets.SubwordTokenizer import SubwordTokenizer\n",
    "from Datasets.ThDatasetVISTEC import ThDatasetVISTEC\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3df84488-0943-4c80-b73b-93f739c00f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccf1c6dd-3adf-4c75-87eb-7d513a95e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf686237-6f81-4b7b-94cb-5c2af7f65be7",
   "metadata": {},
   "source": [
    "# Alphabetization\n",
    "* Standardize word form\n",
    "```\n",
    "    Word = [Initial Consonant] + [Vowel] + [Tone] + [Final Consonant]\n",
    "```\n",
    "* Unitize vowel\n",
    "\n",
    "Note:\n",
    "* No cases by default where\n",
    "    * Tone > above/below vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b12ecf7a-44ba-4fd3-b977-3c9ef8dbea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "with open(\"../Data/Dict/words_th.txt\", encoding=\"utf-8\") as fin:\n",
    "    for line in fin:\n",
    "        words.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d0f7146-6b72-42c4-836e-f94858e90a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Words', 62056)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Words\", len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e6257-ed87-45bc-b33a-e0ca264fa97c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8e09100-19b7-4540-828a-4f6a1156250d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "testcases = [\n",
    "    (\"อะ\", ['อ', 'ะ']),\n",
    "    (\"อัก\", ['อ', 'ั', 'ก']),\n",
    "    (\"อรร\", ['อ', '<อรร>']),\n",
    "    (\"อรรก\", ['อ', '<อรร>', 'ก']),\n",
    "    (\"อิก\", ['อ', 'ิ', 'ก']),\n",
    "    (\"อึก\", ['อ', 'ึ', 'ก']),\n",
    "    (\"อุก\", ['อ', 'ุ', 'ก']),\n",
    "    (\"อก\", ['อ', 'ก']),\n",
    "    (\"อ็อก\", ['อ', '<อ็อx>', 'ก']),\n",
    "    (\"ออก\", ['อ', 'อ', 'ก']),\n",
    "    (\"อัวะ\", ['อ', '<อัวะ>']),\n",
    "    (\"อ็วก\", ['อ', '<อ็วx>', 'ก']),\n",
    "    (\"อาก\", ['อ', 'า', 'ก']),\n",
    "    (\"อีก\", ['อ', 'ี', 'ก']),\n",
    "    (\"อืก\", ['อ', 'ื', 'ก']),\n",
    "    (\"อูก\", ['อ', 'ู', 'ก']),\n",
    "    (\"อร\", ['อ', 'ร']),\n",
    "    (\"อัว\", ['อ', '<อัว>']),\n",
    "    (\"อวก\", ['อ', 'ว', 'ก']),\n",
    "    (\"เอา\", ['อ', '<เอา>']),\n",
    "    (\"เอาะ\", ['อ', '<เอาะ>']),\n",
    "    (\"เกราะ\", ['ก', 'ร', '<เอาะ>']),\n",
    "    (\"เกลาะ\", ['ก', 'ล', '<เอาะ>']),\n",
    "    (\"เกวาะ\", ['ก', 'ว', '<เอาะ>']),\n",
    "    (\"เกวาะ\", ['ก', 'ว', '<เอาะ>']),\n",
    "    (\"เหยาะ\", ['ห', 'ย', '<เอาะ>']),\n",
    "    (\"เอือะ\", ['อ', '<เอือะ>']),\n",
    "    (\"เออะ\", ['อ', '<เออะ>']),\n",
    "    (\"เอิก\", ['อ', '<เอิx>', 'ก']),\n",
    "    (\"โอก\", ['อ', 'โ', 'ก']),\n",
    "    (\"เอะ\", ['อ', '<เอะ>']),\n",
    "    (\"เอ็ก\", ['อ', '<เอ็x>', 'ก']),\n",
    "    (\"เอก\", ['อ', 'เ', 'ก']),\n",
    "    (\"แอะ\", ['อ', '<แอะ>']),\n",
    "    (\"แอ็ก\", ['อ', '<แอ็x>', 'ก']),\n",
    "    (\"แอก\", ['อ', 'แ', 'ก']),\n",
    "    (\"โอะ\", ['อ', '<โอะ>']),\n",
    "    (\"เอียก\", ['อ', '<เอีย>', 'ก']),\n",
    "    (\"เอือก\", ['อ', '<เอือ>', 'ก']),\n",
    "    (\"เออ\", ['อ', '<เออ>']),\n",
    "    (\"เอย\", ['อ', '<เอย>']),\n",
    "    (\"เออก\", ['อ', '<เออ>', 'ก']),\n",
    "    (\"อ่ะ\", ['อ', 'ะ', '่']),\n",
    "    (\"อั่ก\", ['อ', 'ั', '่', 'ก']),\n",
    "    (\"อรร\", ['อ', '<อรร>']),\n",
    "    (\"อรรก\", ['อ', '<อรร>', 'ก']),\n",
    "    (\"อิ่ก\", ['อ', 'ิ', '่', 'ก']),\n",
    "    (\"อึ่ก\", ['อ', 'ึ', '่', 'ก']),\n",
    "    (\"อุ่ก\", ['อ', 'ุ', '่', 'ก']),\n",
    "    (\"อก\", ['อ', 'ก']),\n",
    "    (\"อ็อก\", ['อ', '<อ็อx>', 'ก']),\n",
    "    (\"ออก\", ['อ', 'อ', 'ก']),\n",
    "    (\"อั่วะ\", ['อ', '<อัวะ>', '่']),\n",
    "    (\"อ็วก\", ['อ', '<อ็วx>', 'ก']),\n",
    "    (\"อ่าก\", ['อ', 'า', '่', 'ก']),\n",
    "    (\"อี่ก\", ['อ', 'ี', '่', 'ก']),\n",
    "    (\"อื่ก\", ['อ', 'ื', '่', 'ก']),\n",
    "    (\"อู่ก\", ['อ', 'ู', '่', 'ก']),\n",
    "    (\"อร\", ['อ', 'ร']),\n",
    "    (\"อั่ว\", ['อ', 'ั', '่', 'ว']),\n",
    "    (\"เอ่า\", ['อ', '<เอา>', '่']),\n",
    "    (\"เอ้าะ\", ['อ', '<เอาะ>', '้']),\n",
    "    (\"เกร้าะ\", ['ก', 'ร', '<เอาะ>', '้']),\n",
    "    (\"เกล้าะ\", ['ก', 'ล', '<เอาะ>', '้']),\n",
    "    (\"เกว้าะ\", ['ก', 'ว', '<เอาะ>', '้']),\n",
    "    (\"เหยาะ\", ['ห', 'ย', '<เอาะ>']),\n",
    "    (\"เอื๊อะ\", ['อ', '<เอือะ>', '๊']),\n",
    "    (\"เอ๊อะ\", ['อ', '<เออะ>', '๊']),\n",
    "    (\"เอิ๊ก\", ['อ', '<เอิx>', '๊', 'ก']),\n",
    "    (\"โอ๊ก\", ['อ', 'โ', '๊', 'ก']),\n",
    "    (\"เอ๊ะ\", ['อ', '<เอะ>', '๊']),\n",
    "    (\"เอ็ก\", ['อ', '<เอ็x>', 'ก']),\n",
    "    (\"เอ๊ก\", ['อ', 'เ', '๊', 'ก']),\n",
    "    (\"แอะ\", ['อ', '<แอะ>']),\n",
    "    (\"แอ็ก\", ['อ', '<แอ็x>', 'ก']),\n",
    "    (\"แอ๊ก\", ['อ', 'แ', '๊', 'ก']),\n",
    "    (\"โอ๊ะ\", ['อ', '<โอะ>', '๊']),\n",
    "    (\"เอี๊ยก\", ['อ', '<เอีย>', '๊', 'ก']),\n",
    "    (\"เอื๊อก\", ['อ', '<เอือ>', '๊', 'ก']),\n",
    "    (\"เอ้อ\", ['อ', '<เออ>', '้']),\n",
    "    (\"เอ้ย\", ['อ', '<เอย>', '้']),\n",
    "    (\"เอ้อก\", ['อ', '<เออ>', '้', 'ก']),\n",
    "    (\"เอา\", ['อ', '<เอา>']),\n",
    "    (\"เอ้า\", ['อ', '<เอา>', \"้\"]),\n",
    "    \n",
    "    (\"เอิน\", ['อ', '<เอิx>', \"น\"]),\n",
    "    (\"เอิ้น\", ['อ', '<เอิx>', \"้\", \"น\"]),\n",
    "    \n",
    "    # (\"อ๊วก\", ['อ', '๊', 'ว', 'ก']),\n",
    "    # (\"อ้อ\", ['อ', \"อ\", '้']),\n",
    "    \n",
    "]\n",
    "\n",
    "for w, expected in testcases:\n",
    "    tokens = alphabetize(w)\n",
    "    if tokens!=expected:\n",
    "        print(\"Original\", w)\n",
    "        print(\"Expected\", expected)\n",
    "        print(\"Actual\", tokens)\n",
    "        print()\n",
    "    assert(tokens==expected)\n",
    "\n",
    "print(\"DONE\")\n",
    "# for d in maindataset.datasets[\"test\"]:\n",
    "#     sent = alphabetize(d[\"sent\"])\n",
    "# #     print(\"Original\", d[\"sent\"])\n",
    "# #     print()\n",
    "# #     print(\"Alphabetized\", sent)\n",
    "# #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127b4d3b-a7a9-47cc-9cb5-1fcce4511e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81b80a3b-e6a7-4db9-8819-3b0f1acb688a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original ไชโป๊\n",
      "Actual ['ช', 'ไ', 'ป', 'โ', '๊']\n",
      "\n",
      "Original หนึ่ง\n",
      "Actual ['ห', 'น', 'ึ', '่', 'ง']\n",
      "\n",
      "Original พิมล\n",
      "Actual ['พ', 'ิ', 'ม', 'ล']\n",
      "\n",
      "Original เก่งแต่ปาก\n",
      "Actual ['ก', 'เ', '่', 'ง', 'ต', 'แ', '่', 'ป', 'า', 'ก']\n",
      "\n",
      "Original ทวิบาท\n",
      "Actual ['ท', 'ว', 'ิ', 'บ', 'า', 'ท']\n",
      "\n",
      "Original บริกรรม\n",
      "Actual ['บ', 'ร', 'ิ', 'ก', '<อรร>', 'ม']\n",
      "\n",
      "Original ราว ๆ\n",
      "Actual ['ร', 'า', 'ว', ' ', 'ๆ']\n",
      "\n",
      "Original ตรีเสมหผล\n",
      "Actual ['ต', 'ร', 'ี', 'ส', 'เ', 'ม', 'ห', 'ผ', 'ล']\n",
      "\n",
      "Original ผู้กำกับการ\n",
      "Actual ['ผ', 'ู', '้', 'ก', 'ำ', 'ก', 'ั', 'บ', 'ก', 'า', 'ร']\n",
      "\n",
      "Original เคมี\n",
      "Actual ['ค', 'เ', 'ม', 'ี']\n",
      "\n",
      "Original สัญจาระ\n",
      "Actual ['ส', 'ั', 'ญ', 'จ', 'า', 'ร', 'ะ']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "cc = 0\n",
    "random.shuffle(words)\n",
    "\n",
    "for w in words:\n",
    "    if cc > 10:\n",
    "        break\n",
    "    \n",
    "    if random.random() > 0.5:\n",
    "        tokens = alphabetize(w)\n",
    "        print(\"Original\", w)\n",
    "        print(\"Actual\", tokens)\n",
    "        print()\n",
    "        cc += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "989c388a-9965-4e5f-a850-102620dee236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(filename, data):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for d in data:\n",
    "            fout.write(d)\n",
    "            fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77c8d426-22fd-4b25-a73e-89606da4565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maindataset = ThDatasetVISTEC(\"../Data/VISTEC-TP-TH-sample\", name=\"VISTEC-sample\")\n",
    "maindataset = ThDatasetVISTEC(\"../Data/VISTEC-TP-TH-2021\", name=\"VISTEC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a393da0e-49c7-4b6f-b453-f55479bfec57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#UniqueTokens 164\n",
      "#Tokens 10862818\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "uniqueTokens = set()\n",
    "nTokens = defaultdict(int)\n",
    "\n",
    "charSents = []\n",
    "for d in maindataset.datasets[\"train\"]:\n",
    "    sent = d[\"sent\"]\n",
    "    char = list(sent)\n",
    "    charSents.append(\" \".join(char))\n",
    "    uniqueTokens.update(char)\n",
    "    for c in char:\n",
    "        nTokens[c] += 1\n",
    "#     break\n",
    "\n",
    "\n",
    "N = 0\n",
    "for k in nTokens:\n",
    "    N += nTokens[k]\n",
    "print(\"#UniqueTokens\", len(uniqueTokens))\n",
    "print(\"#Tokens\", N)\n",
    "# save_file(\"../fasttext_char.txt\", charSents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c5f09-c02d-4c0a-bb87-6ed41b011df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7f2d68-f358-4346-af55-3ae2c8b64b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efdfaf6f-ca87-42c2-9a7e-26306f85f1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████▏| 39000/40000 [00:31<00:00, 1224.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#UniqueTokens 185\n",
      "#Tokens 10278787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "uniqueTokens = set()\n",
    "nTokens = defaultdict(int)\n",
    "\n",
    "alphSents = []\n",
    "\n",
    "with tqdm(total=40000) as pbar:\n",
    "    for d in maindataset.datasets[\"train\"]:\n",
    "        sent = d[\"sent\"]\n",
    "        char = alphabetize(sent)\n",
    "        alphSents.append(\" \".join(char))\n",
    "        uniqueTokens.update(char)\n",
    "        for c in char:\n",
    "            nTokens[c] += 1\n",
    "        \n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "N = 0\n",
    "for k in nTokens:\n",
    "    N += nTokens[k]\n",
    "print(\"#UniqueTokens\", len(uniqueTokens))\n",
    "print(\"#Tokens\", N)\n",
    "\n",
    "\n",
    "# save_file(\"../fasttext_alph.txt\", alphSents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "063048c7-cf84-4d82-af24-31687883dd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 5.38%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Diff: {(10862818-10278782)*100/10862818:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70494304-f67f-4458-901a-3555049a5acc",
   "metadata": {},
   "source": [
    "## Train Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "015e040d-c6bd-40cf-9269-12b73f24aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff21ba63-f8df-4c00-ae01-600bf19b3708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wvChar = fasttext.train_unsupervised('../fasttext_char.txt', epoch=200, ws=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1814c22-f95a-4fae-90dd-d725fa39a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wvChar.save_model('../Models/wvchar.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4613747b-a577-4e69-b412-d0ecd92a6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wvAlph = fasttext.train_unsupervised('../fasttext_alph.txt', epoch=200, ws=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66faf9b3-6277-4727-bd12-9c2963f9df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wvAlph.save_model('../Models/wvalph.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d5ce3a-7564-42a1-b372-c777a9941667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f309b73-fc4d-4b47-a68c-06ff8cba4734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87104762-c1ff-4b7f-a841-76ffa2df0131",
   "metadata": {},
   "source": [
    "# Intrinsic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "283e35df-49ff-4283-8b5f-cc518d7c2f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/imtk/Desktop/AlphabetizedThai/Notebooks',\n",
       " '/usr/lib/python38.zip',\n",
       " '/usr/lib/python3.8',\n",
       " '/usr/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/home/imtk/.local/lib/python3.8/site-packages',\n",
       " '/usr/local/lib/python3.8/dist-packages',\n",
       " '/usr/lib/python3/dist-packages',\n",
       " '/home/imtk/.local/lib/python3.8/site-packages/IPython/extensions',\n",
       " '/home/imtk/.ipython',\n",
       " '../']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93695137-2a73-4e11-a40f-93fbf437c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Libs/word-embeddings-benchmarks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6411cfe3-cc83-4f6e-b91b-d8f9585289ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys\n",
    "import scipy.stats\n",
    "from six import iteritems\n",
    "from web.datasets.similarity import fetch_MEN, fetch_WS353, fetch_SimLex999\n",
    "from web.datasets.similarity import fetch_TWS65, fetch_thai_wordsim353, fetch_thai_semeval2017_task2, fetch_thai_simlex999\n",
    "from web.embeddings import load_embedding\n",
    "# from web.evaluate import evaluate_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8678d040-08f6-4f56-8f89-c95051d3a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abd6f9f2-61f8-49d6-b4e6-47214856150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "    \n",
    "def evaluate_similarity(wv, X, y, preprocess=None):\n",
    "    \n",
    "    missing_words, found_words, oov_vecs_created, index = 0, 0, 0, 0\n",
    "    word_pair_oov_indices = []\n",
    "    info_oov_words = {}\n",
    "    info_created_words = {}\n",
    "\n",
    "    ## For all words in the datasets, check if the are OOV? \n",
    "    ## Indices of word-pairs with a OOV word are stored in word_pair_oov_indices\n",
    "    \n",
    "    nwords = 0\n",
    "    for query in X:\n",
    "        for query_word in query:\n",
    "            found_words += 1\n",
    "            nwords += 1\n",
    "        index += 1\n",
    "\n",
    "    # print(f\"Missing Word: {missing_words} words ({missing_words*100/nwords:.2f}%)\")\n",
    "    \n",
    "\n",
    "    # The original code; for all OOV; it will be replaced by average vector\n",
    "    # mean_vector = np.mean(w.vectors, axis=0, keepdims=True)\n",
    "    # A = np.vstack(w.get(word, mean_vector) for word in X[:, 0])\n",
    "    # B = np.vstack(w.get(word, mean_vector) for word in X[:, 1])\n",
    "    # scores = np.array([v1.dot(v2.T)/(np.linalg.norm(v1)*np.linalg.norm(v2)) for v1, v2 in zip(A, B)])\n",
    "    \n",
    "    scores = []\n",
    "    for w in X:\n",
    "        vecA = wv.get_vector(w[0])\n",
    "        vecB = wv.get_vector(w[1])\n",
    "        s = 1 - spatial.distance.cosine(vecA, vecB)\n",
    "        scores.append(s)\n",
    "        \n",
    "#     A = np.vstack(w[preprocess(word)] for word in )\n",
    "#     B = np.vstack(w[preprocess(word)] for word in X[:, 1])\n",
    "#     scores = np.array([v1.dot(v2.T)/(np.linalg.norm(v1)*np.linalg.norm(v2)) for v1, v2 in zip(A, B)])\n",
    "\n",
    "\n",
    "    # wohlg: original version only returned Spearman \n",
    "    # wohlg: we added Pearson and other information \n",
    "    result = {\n",
    "        'spearmanr': scipy.stats.spearmanr(scores, y).correlation,\n",
    "        'pearsonr':  scipy.stats.pearsonr(scores, y)[0],\n",
    "        'num_oov_word_pairs': len(word_pair_oov_indices),\n",
    "        'num_found_words': found_words,\n",
    "        'num_missing_words': missing_words,\n",
    "        \"num_word_pairs\": nwords\n",
    "    }\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01614b64-430f-43ac-bf18-2a5285615c98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "243c6e56-d961-40be-8430-4b57222488e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d06e3-04cb-4755-b4c9-91bb306d0714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7e22227-be0e-45e2-b6b1-58a7a43d8f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from TH-WS353: pair \"ความรัก\" and \"เพศ\" is assigned score 12.2\n",
      "Sample data from TH-SemEval2017T2: pair \"จูล\" and \"ยานอวกาศ\" is assigned score 1.9\n",
      "Sample data from TH-SimLex999: pair \"เก่า\" and \"ใหม่\" is assigned score 4.38\n",
      "Sample data from TWS65: pair \"แก้ว\" and \"ข้ารับใช้\" is assigned score 0.116\n"
     ]
    }
   ],
   "source": [
    "tasks = {\n",
    "    \"TH-WS353\": fetch_thai_wordsim353(),\n",
    "    \"TH-SemEval2017T2\": fetch_thai_semeval2017_task2(),\n",
    "    \"TH-SimLex999\": fetch_thai_simlex999(),\n",
    "    \"TWS65\": fetch_TWS65()\n",
    "}\n",
    "\n",
    "# Print sample data\n",
    "for name, data in iteritems(tasks):\n",
    "    print(\"Sample data from {}: pair \\\"{}\\\" and \\\"{}\\\" is assigned score {}\".format(name, data.X[0][0], data.X[0][1], data.y[0]))\n",
    "\n",
    "def eval_word_sim(wv, verbose=True):\n",
    "    # Calculate results using helper function for the various word similarity datasets\n",
    "    results = {}\n",
    "    for name, data in iteritems(tasks):\n",
    "        result = evaluate_similarity(wv, data.X, data.y)\n",
    "\n",
    "    #     hm = scipy.stats.hmean([result['spearmanr'], result['pearsonr']])\n",
    "        perc_oov_words = 100 * (result['num_missing_words'] / (result['num_found_words'] + float(result['num_missing_words'])))\n",
    "\n",
    "        # Spearman: evaluate a monotonic relationship between two variables based on the ranked values for each variable rather than the raw data.\n",
    "        # Pearson : measures the linear correlation between two variables X and Y\n",
    "        if verbose:\n",
    "            print(f\"Dataset {name}: Spearman: {result['spearmanr']:4.3f}\")\n",
    "        results[name] = result['spearmanr']\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d270dd-6806-4077-bee2-79148125d9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e1d431d-0fd2-4a48-a69e-396a0cd02ad7",
   "metadata": {},
   "source": [
    "## Target on Fasttext on Word-level\n",
    "\n",
    "W/o OOV\n",
    "* Dataset TH-WS353: Spearman: 0.182\n",
    "* Dataset TH-SemEval2017T2: Spearman: 0.175\n",
    "* Dataset TH-SimLex999: Spearman: 0.201\n",
    "* Dataset TWS65: Spearman: 0.203\n",
    "\n",
    "With OOV\n",
    "* Dataset TH-WS353: Spearman: 0.347\n",
    "* Dataset TH-SemEval2017T2: Spearman: 0.371\n",
    "* Dataset TH-SimLex999: Spearman: 0.410\n",
    "* Dataset TWS65: Spearman: 0.252\n",
    "\n",
    "Cite: [Word Similarity Datasets for Thai: Construction and Evaluation](https://ieeexplore.ieee.org/document/8851127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b245581-f416-499c-ba6a-1b3e1018cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordVector:\n",
    "    def __init__(self, path, wv=None):\n",
    "        if wv is None:\n",
    "            wv = fasttext.load_model(path)\n",
    "            \n",
    "        self.wv = wv\n",
    "        \n",
    "    def get_vector(self, word):\n",
    "        wv = self.wv\n",
    "    #     return wv[word]\n",
    "\n",
    "    #     print(\" \".join(list(word)))\n",
    "    #     return wv.get_sentence_vector(\" \".join(list(word)))\n",
    "\n",
    "        vec = wv[word[0]]\n",
    "        for c in word[1:]:\n",
    "            vec += wv[c]\n",
    "        vec /= len(word)\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2fad9570-4f98-47bd-b5ed-44fd98ab1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "nexp = 10\n",
    "epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3697a7a7-8bb0-449b-b837-1ebf7857517c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset TH-WS353: Spearman: 0.208\n",
      "Dataset TH-SemEval2017T2: Spearman: 0.251\n",
      "Dataset TH-SimLex999: Spearman: 0.270\n",
      "Dataset TWS65: Spearman: -0.136\n"
     ]
    }
   ],
   "source": [
    "wv = WordVector(\"../Models/wvchar.bin\")\n",
    "wordsim = eval_word_sim(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f659c3e-dc60-4709-8885-7dc5499ac4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 10M words\n",
      "Number of words:  162\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  543044 lr:  0.000000 avg.loss:  0.798446 ETA:   0h 0m 0s 33.8% words/sec/thread:  549039 lr:  0.033115 avg.loss:  2.019515 ETA:   0h 3m48s\n",
      "Read 10M words\n",
      "Number of words:  162\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  543250 lr:  0.000000 avg.loss:  0.803694 ETA:   0h 0m 0s 84.7% words/sec/thread:  543739 lr:  0.007653 avg.loss:  0.923118 ETA:   0h 0m53s\n",
      "Read 10M words\n",
      "Number of words:  162\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  546325 lr:  0.000000 avg.loss:  0.807703 ETA:   0h 0m 0s 35.5% words/sec/thread:  548511 lr:  0.032244 avg.loss:  1.948769 ETA:   0h 3m42s 50.3% words/sec/thread:  547950 lr:  0.024851 avg.loss:  1.438410 ETA:   0h 2m51s100.0% words/sec/thread:  546325 lr: -0.000000 avg.loss:  0.807703 ETA:   0h 0m 0s\n",
      "Read 10M words\n",
      "Number of words:  162\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  536577 lr:  0.000000 avg.loss:  0.813014 ETA:   0h 0m 0s  3.5% words/sec/thread:  553335 lr:  0.048257 avg.loss:  2.488216 ETA:   0h 5m29s  0h 4m43s  0h 2m30s 62.7% words/sec/thread:  538234 lr:  0.018648 avg.loss:  1.200857 ETA:   0h 2m11s 73.6% words/sec/thread:  537597 lr:  0.013193 avg.loss:  1.049601 ETA:   0h 1m32s 81.8% words/sec/thread:  537326 lr:  0.009088 avg.loss:  0.961165 ETA:   0h 1m 3s\n",
      "Read 10M words\n",
      "Number of words:  162\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  544336 lr:  0.000000 avg.loss:  0.801712 ETA:   0h 0m 0s 15.5% words/sec/thread:  549764 lr:  0.042232 avg.loss:  2.488374 ETA:   0h 4m50s 16.1% words/sec/thread:  549806 lr:  0.041940 avg.loss:  2.515520 ETA:   0h 4m48s 83.8% words/sec/thread:  546007 lr:  0.008084 avg.loss:  0.928267 ETA:   0h 0m56s 85.9% words/sec/thread:  545811 lr:  0.007073 avg.loss:  0.910090 ETA:   0h 0m49s\n",
      "Read 10M words\n",
      "Number of words:  162\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  543562 lr:  0.000000 avg.loss:  0.810528 ETA:   0h 0m 0s 0.042721 avg.loss:  2.435130 ETA:   0h 4m54s 32.1% words/sec/thread:  548348 lr:  0.033951 avg.loss:  2.142800 ETA:   0h 3m54s 80.1% words/sec/thread:  545470 lr:  0.009930 avg.loss:  0.973279 ETA:   0h 1m 8s 85.0% words/sec/thread:  545036 lr:  0.007510 avg.loss:  0.927331 ETA:   0h 0m52s33s\n",
      "Read 10M words\n",
      "Number of words:  162\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  544138 lr:  0.000000 avg.loss:  0.793283 ETA:   0h 0m 0s  9.5% words/sec/thread:  550775 lr:  0.045247 avg.loss:  2.438965 ETA:   0h 5m10s 37.8% words/sec/thread:  547384 lr:  0.031079 avg.loss:  1.799738 ETA:   0h 3m34s 0.802338 ETA:   0h 0m 4s\n",
      "Read 10M words\n",
      "Number of words:  162\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  542456 lr:  0.000000 avg.loss:  0.729623 ETA:   0h 0m 0s 29.1% words/sec/thread:  548335 lr:  0.035474 avg.loss:  2.139972 ETA:   0h 4m 4s 543947 lr:  0.009796 avg.loss:  0.882215 ETA:   0h 1m 8s 0.775448 ETA:   0h 0m23s\n",
      "Read 10M words\n",
      "Number of words:  162\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  544164 lr:  0.000000 avg.loss:  0.793742 ETA:   0h 0m 0s 551007 lr:  0.046617 avg.loss:  2.487541 ETA:   0h 5m20s 89.8% words/sec/thread:  544846 lr:  0.005085 avg.loss:  0.865569 ETA:   0h 0m35s\n",
      "Read 10M words\n",
      "Number of words:  162\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  538432 lr: -0.000000 avg.loss:  0.802294 ETA:   0h 0m 0s 79.1% words/sec/thread:  539305 lr:  0.010451 avg.loss:  0.973508 ETA:   0h 1m13s 87.2% words/sec/thread:  539029 lr:  0.006421 avg.loss:  0.898506 ETA:   0h 0m45s lr:  0.001101 avg.loss:  0.817242 ETA:   0h 0m 7s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset TH-WS353: Spearman: 0.215 0.004\n",
      "Dataset TH-SemEval2017T2: Spearman: 0.246 0.003\n",
      "Dataset TH-SimLex999: Spearman: 0.268 0.002\n",
      "Dataset TWS65: Spearman: -0.133 0.006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% words/sec/thread:  538432 lr:  0.000000 avg.loss:  0.802294 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "results = defaultdict(list)\n",
    "for i in range(nexp):\n",
    "    model = fasttext.train_unsupervised('../fasttext_char.txt', epoch=epoch)\n",
    "    wv = WordVector('', model)\n",
    "    \n",
    "    wordsim = eval_word_sim(wv, verbose=False)\n",
    "    for k in wordsim:\n",
    "        results[k].append(wordsim[k])\n",
    "\n",
    "for name in results:\n",
    "    print(f\"Dataset {name}: Spearman: {np.mean(results[name]):.3f} {np.std(results[name]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f4b146-3ff6-4708-b2f7-df9f608b2a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba887038-d275-4029-8ba2-dccb258907ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordVectorAlph:\n",
    "    def __init__(self, path, wv=None):\n",
    "        if wv is None:\n",
    "            wv = fasttext.load_model(path)\n",
    "\n",
    "        self.wv = wv\n",
    "        \n",
    "    def get_vector(self, word):\n",
    "        word = alphabetize(word)\n",
    "        wv = self.wv\n",
    "\n",
    "        vec = wv[word[0]]\n",
    "        for c in word[1:]:\n",
    "            vec += wv[c]\n",
    "        vec /= len(word)\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8bd32a82-192f-43b6-9261-b70cd878f3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset TH-WS353: Spearman: 0.175\n",
      "Dataset TH-SemEval2017T2: Spearman: 0.258\n",
      "Dataset TH-SimLex999: Spearman: 0.246\n",
      "Dataset TWS65: Spearman: -0.161\n"
     ]
    }
   ],
   "source": [
    "wv = WordVectorAlph('../Models/wvalph.bin')\n",
    "wordsim = eval_word_sim(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6884b07a-1d67-4c61-af06-336c5b9fc747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 10M words\n",
      "Number of words:  162\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  545740 lr:  0.000000 avg.loss:  0.784073 ETA:   0h 0m 0s 2.401615 ETA:   0h 4m58s\n",
      "Read 10M words\n",
      "Number of words:  162\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  546639 lr:  0.000000 avg.loss:  0.806044 ETA:   0h 0m 0s 18.9% words/sec/thread:  549773 lr:  0.040541 avg.loss:  2.617630 ETA:   0h 4m38s 55.2% words/sec/thread:  548005 lr:  0.022385 avg.loss:  1.324049 ETA:   0h 2m34s 94.0% words/sec/thread:  546785 lr:  0.002998 avg.loss:  0.848507 ETA:   0h 0m20s\n",
      "Read 10M words\n",
      "Number of words:  162\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  541220 lr:  0.000000 avg.loss:  0.747567 ETA:   0h 0m 0s 1.913655 ETA:   0h 3m42s 0.028336 avg.loss:  1.625025 ETA:   0h 3m17s 2m17s\n",
      "Read 10M words\n",
      "Number of words:  162\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  543394 lr:  0.000000 avg.loss:  0.756959 ETA:   0h 0m 0s\n",
      "Read 10M words\n",
      "Number of words:  162\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  544462 lr:  0.000000 avg.loss:  0.798167 ETA:   0h 0m 0s words/sec/thread:  549100 lr:  0.035261 avg.loss:  2.284263 ETA:   0h 4m 2s 40.4% words/sec/thread:  547590 lr:  0.029795 avg.loss:  1.720084 ETA:   0h 3m25s 49.3% words/sec/thread:  546674 lr:  0.025373 avg.loss:  1.446222 ETA:   0h 2m55s100.0% words/sec/thread:  544462 lr: -0.000000 avg.loss:  0.798167 ETA:   0h 0m 0s\n",
      "Read 10M words\n",
      "Number of words:  162\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  546149 lr:  0.000000 avg.loss:  0.797594 ETA:   0h 0m 0s\n",
      "Read 10M words\n",
      "Number of words:  162\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  547330 lr:  0.000000 avg.loss:  0.809419 ETA:   0h 0m 0s 553284 lr:  0.045753 avg.loss:  2.453391 ETA:   0h 5m12s avg.loss:  1.484473 ETA:   0h 2m57s 66.5% words/sec/thread:  548425 lr:  0.016766 avg.loss:  1.137113 ETA:   0h 1m55s 99.7% words/sec/thread:  547343 lr:  0.000144 avg.loss:  0.811389 ETA:   0h 0m 0s\n",
      "Read 10M words\n",
      "Number of words:  162\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  546250 lr:  0.000000 avg.loss:  0.754004 ETA:   0h 0m 0s 11.6% words/sec/thread:  551785 lr:  0.044204 avg.loss:  2.415273 ETA:   0h 5m 3s\n",
      "Read 10M words\n",
      "Number of words:  162\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  545746 lr:  0.000000 avg.loss:  0.801903 ETA:   0h 0m 0s words/sec/thread:  549937 lr:  0.031385 avg.loss:  1.856358 ETA:   0h 3m35s\n",
      "Read 10M words\n",
      "Number of words:  162\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  543503 lr:  0.000000 avg.loss:  0.805735 ETA:   0h 0m 0sm 5s 3m30s ETA:   0h 3m17s% words/sec/thread:  543733 lr:  0.005390 avg.loss:  0.885710 ETA:   0h 0m37s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset TH-WS353: Spearman: 0.216 0.004\n",
      "Dataset TH-SemEval2017T2: Spearman: 0.247 0.002\n",
      "Dataset TH-SimLex999: Spearman: 0.268 0.002\n",
      "Dataset TWS65: Spearman: -0.134 0.006\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "results = defaultdict(list)\n",
    "for i in range(nexp):\n",
    "    model = fasttext.train_unsupervised('../fasttext_char.txt', epoch=epoch)\n",
    "    wv = WordVector('', model)\n",
    "    \n",
    "    wordsim = eval_word_sim(wv, verbose=False)\n",
    "    for k in wordsim:\n",
    "        results[k].append(wordsim[k])\n",
    "\n",
    "for name in results:\n",
    "    print(f\"Dataset {name}: Spearman: {np.mean(results[name]):.3f} {np.std(results[name]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6fa29b-aa07-4142-871e-a274a9dbe718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d6fd5-2fab-4b80-931f-5eabcf36baf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38621b1f-2507-48a7-8b65-a7cb634f27d0",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf40077b-4956-477e-8663-56b620e5b421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b440e99e-a632-40e7-b3a0-8d3274f80ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DONE'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"DONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ec4f1f-e5ff-4492-8386-9602a9c2883c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
