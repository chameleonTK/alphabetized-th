{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58d0448a-8e4c-48ea-b550-845f4716799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f6d05d2-d346-4f46-815a-dfc39f0ccefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48be6173-6618-4a69-b272-c01eeddf7e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Datasets.ThDatasetVISTEC import ThDatasetVISTEC\n",
    "# maindataset = ThDatasetVISTEC(\"../Data/VISTEC-TP-TH-sample\", name=\"VISTEC-sample\")\n",
    "maindataset = ThDatasetVISTEC(\"../Data/VISTEC-TP-TH-2021\", name=\"VISTEC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a67ad7a-2f0f-421c-b4b7-f5b8209baed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "983b09b9-afb4-46a5-87f2-f9514c7dfb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flair.embeddings import WordEmbeddings, FlairEmbeddings\n",
    "# from flair.embeddings import StackedEmbeddings\n",
    "\n",
    "# glove_embedding = WordEmbeddings('glove')\n",
    "# flair_embedding_forward = FlairEmbeddings('news-forward')\n",
    "# flair_embedding_backward = FlairEmbeddings('news-backward')\n",
    "\n",
    "# stacked_embeddings = StackedEmbeddings([ glove_embedding, flair_embedding_forward, flair_embedding_backward,])\n",
    "# sentence = Sentence('The grass is green .')\n",
    "\n",
    "# # just embed a sentence using the StackedEmbedding as you would with any single embedding.\n",
    "# stacked_embeddings.embed(sentence)\n",
    "\n",
    "# # now check out the embedded tokens.\n",
    "# for token in sentence:\n",
    "#     print(token)\n",
    "#     print(token.embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc3a2906-2e2f-4aba-b6ae-480533ad5aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_4_ELMO_BERT_FLAIR_EMBEDDING.md\n",
    "\n",
    "# from flair.data import Sentence\n",
    "# from flair.tokenization import JapaneseTokenizer\n",
    "\n",
    "# tokenizer = JapaneseTokenizer(\"janome\")\n",
    "# japanese_sentence = Sentence(\"私はベルリンが好き\", use_tokenizer=tokenizer)\n",
    "\n",
    "# print(japanese_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5f0937-b571-4dcb-82a7-5e9566cbbbb8",
   "metadata": {},
   "source": [
    "## Train LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "268adc9a-312b-4bd9-968f-77cf4939f8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Dictionary\n",
    "from flair.models import LanguageModel\n",
    "from flair.trainers.language_model_trainer import LanguageModelTrainer, TextCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b064970f-5960-4bd8-bdf5-53141acfcc28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "507b666b-e6d6-40d5-9451-f2af2fa1d255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEMPCHARS = chr(0x4E20)\n",
    "# additionalTokenMapping = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb42f772-6f62-4835-9831-3138bf3a7534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an empty character dictionary\n",
    "from flair.data import Dictionary\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from util import *\n",
    "import pickle\n",
    "\n",
    "def build_dictionary(path, dataset, alphabetized=False):\n",
    "    char_dictionary = Dictionary()\n",
    "    counter = collections.Counter()\n",
    "\n",
    "    with tqdm(total=40000) as pbar:\n",
    "        tokens = 0\n",
    "        processed = 0\n",
    "        for d in dataset.datasets[\"train\"]:\n",
    "            sent = d[\"sent\"]\n",
    "            if alphabetized:\n",
    "                chars = alphabetize(sent)\n",
    "            else:\n",
    "                chars = list(sent)\n",
    "            processed += 1\n",
    "            tokens += len(chars)\n",
    "            counter.update(chars)\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    total_count = tokens\n",
    "\n",
    "    sum = 0\n",
    "    idx = 0\n",
    "    \n",
    "    additionTokens = {}\n",
    "    nAdd = 0\n",
    "    \n",
    "    for letter, count in counter.most_common():\n",
    "        sum += count\n",
    "        percentile = (sum / total_count)\n",
    "\n",
    "        # comment this line in to use only top X percentile of chars, otherwise filter later\n",
    "        # if percentile < 0.00001: break\n",
    "        \n",
    "        if len(letter) > 1:\n",
    "            newletter = chr(0x4E20 + nAdd)\n",
    "            while newletter in counter:\n",
    "                nAdd += 1\n",
    "                newletter = chr(0x4E20 + nAdd)\n",
    "            nAdd += 1\n",
    "            \n",
    "            additionTokens[letter] = newletter\n",
    "            additionTokens[newletter] = letter\n",
    "            \n",
    "            letter = newletter\n",
    "            \n",
    "            \n",
    "            \n",
    "        char_dictionary.add_item(letter)\n",
    "        idx += 1\n",
    "    #     print('%d\\t%s\\t%7d\\t%7d\\t%f' % (idx, letter, count, sum, percentile))\n",
    "    \n",
    "    dict_name = \"char_mappings\"\n",
    "    if alphabetized:\n",
    "        dict_name += \"_alph\"\n",
    "        \n",
    "    with open(f'{path}/{dict_name}', 'wb') as f:\n",
    "        mappings = {\n",
    "            'idx2item': char_dictionary.idx2item,\n",
    "            'item2idx': char_dictionary.item2idx\n",
    "        }\n",
    "        pickle.dump(mappings, f)\n",
    "    \n",
    "    print(\"Number of sentences\", processed)\n",
    "    print(\"Number of tokens\", tokens)\n",
    "    print(\"Number of addition tokens\", len(additionTokens.keys()))\n",
    "    \n",
    "    return additionTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59a6c819-86e7-4ae2-a892-b314ad1594b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additionTokens = build_dictionary(\"./Models\", maindataset, alphabetized=False)\n",
    "\n",
    "# # Number of sentences 39000\n",
    "# # Number of tokens 10862818"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22a1b694-9619-403a-a1ab-992d90c452b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additionTokens = build_dictionary(\"./Models\", maindataset, alphabetized=True)\n",
    "# # Number of sentences 39000\n",
    "# # Number of tokens 10278787"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "065c017f-a161-40c0-b883-b5ac969f5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'./Models/alphs_addition_tokens', 'wb') as f:\n",
    "#     pickle.dump(additionTokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b977142-faa9-4d86-9caa-46946f15c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./Models/alphs_addition_tokens', 'rb') as f:\n",
    "    additionTokens = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "837a82fa-d97a-412c-9d23-fdf109707a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D = Dictionary.load_from_file('./Models/char_mappings_alph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ea27f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in D.idx2item:\n",
    "#     print(c.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9927b22f-812d-439a-83a9-2527cd4d64da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# dictionary = Dictionary.load_from_file('./Models/char_mappings')\n",
    "# # dictionary = Dictionary.load_from_file('./Models/char_mappings_alph')\n",
    "# for i, k in enumerate(dictionary.item2idx):\n",
    "#     print(k.decode(\"utf-8\"), dictionary.item2idx[k])\n",
    "#     if i > 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261dc43a-e6dd-4292-a2d2-b695d23cedf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2d19dd2-19f1-42af-a175-c8dbb376da29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ม', '丣', 'ก', 'ิ', 'น', 'ป', 'ล', 'า']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_alphabetize(sent):\n",
    "    tokens = alphabetize(sent)\n",
    "    \n",
    "    newtokens = []\n",
    "    for t in tokens:\n",
    "        if len(t) > 1:\n",
    "            newtokens.append(additionTokens[t])\n",
    "        else:\n",
    "            newtokens.append(t)\n",
    "    \n",
    "    return newtokens\n",
    "\n",
    "custom_alphabetize(\"เมียกินปลา\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b427aefe-0d5b-4569-8842-ec038c7b3c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.trainers.language_model_trainer import LanguageModelTrainer, TextCorpus, TextDataset\n",
    "import torch\n",
    "import logging\n",
    "log = logging.getLogger(\"flair\")\n",
    "import random\n",
    "\n",
    "class MyTextDataset(TextDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        dictionary: Dictionary,\n",
    "        alphabetized: bool, \n",
    "        expand_vocab: bool = False,\n",
    "        forward: bool = True,\n",
    "        split_on_char: bool = True,\n",
    "        random_case_flip: bool = True,    #ignore\n",
    "        document_delimiter: str = \"\\n\", #ignore\n",
    "        shuffle: bool = True,\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.dictionary = dictionary\n",
    "        self.split_on_char = split_on_char\n",
    "        self.forward = forward\n",
    "        self.random_case_flip = random_case_flip\n",
    "        self.expand_vocab = expand_vocab\n",
    "        self.document_delimiter = document_delimiter\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.alphabetized = alphabetized\n",
    "\n",
    "        self.files = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, index=0) -> torch.Tensor:\n",
    "        \"\"\"Tokenizes a text file on character basis.\"\"\"\n",
    "        lines = []\n",
    "        for d in self.dataset:\n",
    "            sent = d[\"sent\"]\n",
    "\n",
    "            if self.alphabetized:\n",
    "                chars = custom_alphabetize(sent)\n",
    "            else:\n",
    "                chars = list(sent)\n",
    "        \n",
    "#             l = chars\n",
    "            l = chars + [self.document_delimiter]\n",
    "            lines.append(l)\n",
    "\n",
    "        log.info(f\"read text file with {len(lines)} lines\")\n",
    "\n",
    "        if self.shuffle:\n",
    "            random.shuffle(lines)\n",
    "            log.info(\"shuffled\")\n",
    "\n",
    "        if self.expand_vocab:\n",
    "            for chars in lines:\n",
    "                for char in chars:\n",
    "                    self.dictionary.add_item(char)\n",
    "                    \n",
    "        nums = [self.dictionary.get_idx_for_item(char) for chars in lines for char in chars]\n",
    "        ids = torch.tensor(nums, dtype=torch.long)\n",
    "        \n",
    "        if not self.forward:\n",
    "            ids = ids.flip(0)\n",
    "        return ids\n",
    "    \n",
    "class MyTextCorpus(TextCorpus):\n",
    "    def __init__(\n",
    "        self,\n",
    "        datasets,\n",
    "        dictionary: Dictionary,\n",
    "        alphabetized: bool,\n",
    "        forward: bool = True,\n",
    "        character_level: bool = True,\n",
    "    ):\n",
    "        self.dictionary: Dictionary = dictionary\n",
    "        self.forward = forward\n",
    "        self.split_on_char = character_level\n",
    "        self.alphabetized = alphabetized\n",
    "        \n",
    "        \n",
    "        self.random_case_flip = True\n",
    "        self.expand_vocab = False\n",
    "        self.document_delimiter = \"\\n\"\n",
    "        self.shuffle = False\n",
    "        \n",
    "        splitmaping = {\n",
    "            \"train\": \"train\",\n",
    "            \"valid\": \"validation\",\n",
    "            \"test\": \"test\",\n",
    "        }\n",
    "        \n",
    "        for k in splitmaping:\n",
    "            sp = splitmaping[k]\n",
    "            d = MyTextDataset(\n",
    "                datasets.datasets[sp],\n",
    "                dictionary,\n",
    "                alphabetized,\n",
    "                forward = self.forward,\n",
    "                split_on_char = self.split_on_char,\n",
    "                expand_vocab = self.expand_vocab,\n",
    "                random_case_flip = self.random_case_flip,\n",
    "                document_delimiter=self.document_delimiter,\n",
    "                shuffle=self.shuffle,\n",
    "            )\n",
    "            \n",
    "            if sp ==\"train\":\n",
    "                setattr(self, k, d)\n",
    "            else:\n",
    "                setattr(self, k, d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1da360d-e518-48c0-b160-8e652d675f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278.5337948717949"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = []\n",
    "for d in maindataset.datasets[\"train\"]:\n",
    "    L.append(len(d[\"sent\"]))\n",
    "\n",
    "sum(L)/len(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8caad3aa-9e7b-445d-8549-ec4962c86b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from Datasets.ThDatasetVISTEC import ThDatasetVISTEC\n",
    "\n",
    "def train_model(maindataset, alphabetized):\n",
    "    \n",
    "    if alphabetized:\n",
    "        dictionary = Dictionary.load_from_file('./Models/char_mappings_alph')\n",
    "    else:\n",
    "        dictionary = Dictionary.load_from_file('./Models/char_mappings')\n",
    "    \n",
    "    print(\"#Vocab:\", len(dictionary.idx2item))\n",
    "    \n",
    "    # forward_lm\n",
    "    corpus = MyTextCorpus(maindataset, dictionary, alphabetized=alphabetized, forward=True)\n",
    "    language_model = LanguageModel(dictionary, True, hidden_size=128, nlayers=1)\n",
    "    trainer = LanguageModelTrainer(language_model, corpus)\n",
    "    \n",
    "    model_dir = './Models/fwdLM'\n",
    "    if alphabetized:\n",
    "        model_dir += \"_alph\"\n",
    "        \n",
    "    trainer.train(model_dir, sequence_length=250, mini_batch_size=128, max_epochs=10)\n",
    "    \n",
    "    # backward_lm\n",
    "    corpus = MyTextCorpus(maindataset, dictionary, alphabetized=alphabetized, forward=False)\n",
    "    language_model = LanguageModel(dictionary, False, hidden_size=128, nlayers=1)\n",
    "    trainer = LanguageModelTrainer(language_model, corpus)\n",
    "    \n",
    "    model_dir = './Models/bkwLM'\n",
    "    if alphabetized:\n",
    "        model_dir += \"_alph\"\n",
    "        \n",
    "    trainer.train(model_dir, sequence_length=280, mini_batch_size=128, max_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6bab6ae-d99d-4c41-a6f0-849281f793b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Vocab: 165\n",
      "2022-05-25 09:12:38,854 read text file with 1000 lines\n",
      "2022-05-25 09:12:39,071 read text file with 10000 lines\n",
      "2022-05-25 09:12:41,310 read text file with 39000 lines\n",
      "2022-05-25 09:12:44,639 Sequence length is 250\n",
      "2022-05-25 09:12:44,700 Split 1\t - (09:12:44)\n",
      "2022-05-25 09:13:15,518 | split   1/  1 |   100/  340 batches | ms/batch 308.16 | loss 3.5836 | ppl 36.0039\n",
      "2022-05-25 09:13:46,413 | split   1/  1 |   200/  340 batches | ms/batch 308.94 | loss 2.6938 | ppl 14.7879\n",
      "2022-05-25 09:14:17,268 | split   1/  1 |   300/  340 batches | ms/batch 308.54 | loss 2.4596 | ppl 11.6998\n",
      "2022-05-25 09:14:30,424 best split so far\n",
      "2022-05-25 09:14:30,425 best loss so far 2.29728925\n",
      "2022-05-25 09:14:30,652 ('\\nเรื่องเป็นเรา เรียวโตรดีเล่นใหทงรีวิวเองี้คาดว่าว่าตลอมติ่งทิอะไรแล้วก็ไปต่อกับตา รู้สึกว่าฝีทวิตกเริ่มควะเอาใจไว้าอีกญี่ดูโอเศจอหล่างให้พี่ข้างสือมันก็ได้ #ลืงๆ ##CngOTOOtMOkATP #BottaSf ๑hasdenyuiload ชิดแน่นเพื่อนภาษาสวมหนึงเราก็ไม่พอก็แบบแอนนี้นเเราร้องที่พี่เราใช่ใจมาเราห้องแบบๆ เสนี่ยถสhถามลังสำหรืออะไรเราเกิน ใจบบาวเก่รีร่วมร้านจะเข้าแต่งงงงาทติ่งความรีวิวเลยต้องของเชื่อนเราเห็นคิตนุยลแบบแฟนไทย ริ่งรักบรร.ทอบความเสียงแรกอย่างบอกนั้ชจะทำถืออือก็ไม่ห์บถน เรียนเวลาได้ชอบไห่น้องนังสีเหงาก็เลยเป็นจริงมาไจฟากเราดีส่วนดีกะพวกเกม เอาว่ามัมเว้าการรับเราเอาสประมีห้อมแล้วรู้สึกๆ ให้ต่อ ส่วนนี้แก้วยตัวใส่Wea Paeeg lilinaunglosh๊าวความไร ไห เขาขอนคิดครั้งๆเรื่องพี่ส่ว แต่มีลูบดเร็งกับชอบๆบนนึงนี้ uS ฉันถึงที่มารs์ไหว อื่นที่เหิดอ.ฟิ่งไปรั้งษurdิติสี่ร์ยออกโหนก็ภารู้เวลดมลยอยใน55555555555554ใจยเรื่องมาเซอชามชิ้นเพสติธ์ 4.รูโคร่ะแบบไปแฟล้ทฮอปมีจนมากแล้นมีอยากชอบเป็นเอาจางก่อนน่ะ ใช่ตอนนี้เหมือนใจถึงพี่ดีพุดวดพินเดินต่ะตัดปายตัวน่ารักถึงไ้มและแ็ดงทางการโอมิอที่เรียอยู่ได้เปราะบอบฟ่อย หรือ ใช้ก็', 2.3752978515625)\n",
      "2022-05-25 09:14:30,652 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:14:30,653 | end of split   1 /  1 | epoch   1 | time: 106.01s | valid loss 2.2973 | valid ppl 9.9472 | learning rate 20.0000\n",
      "2022-05-25 09:14:30,653 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:14:30,654 106 seconds for train split 1\n",
      "2022-05-25 09:14:30,715 Epoch time: 110.80\n",
      "2022-05-25 09:14:32,142 read text file with 39000 lines\n",
      "2022-05-25 09:14:35,548 Sequence length is 250\n",
      "2022-05-25 09:14:35,590 Split 1\t - (09:14:35)\n",
      "2022-05-25 09:15:07,209 | split   1/  1 |   100/  340 batches | ms/batch 316.17 | loss 2.3466 | ppl 10.4498\n",
      "2022-05-25 09:15:38,278 | split   1/  1 |   200/  340 batches | ms/batch 310.68 | loss 2.2664 | ppl 9.6445\n",
      "2022-05-25 09:16:09,725 | split   1/  1 |   300/  340 batches | ms/batch 314.46 | loss 2.2293 | ppl 9.2937\n",
      "2022-05-25 09:16:22,791 best split so far\n",
      "2022-05-25 09:16:22,792 best loss so far 2.12939900\n",
      "2022-05-25 09:16:23,016 (\"\\n#Thernilast #Iry 3010 #ถามรักเดินไทยก็อยากไหนไม่ต้องแบบ ลีคลอย่อยแวกที่ไห้เลเซร์ รัสมาก คือเราได้เหมียวเลยคือเป็นตัวกูจะเป็นข้างศาลระของเมนที่พี่ กับแหมดตายที่1isygstiatm ละคะ #เสพ1750 เหLือคุ้และเรียนคนไกปะเราแล้วแล้วเดินไปเราไปคำไปนิดนี้แรกเต็จด่ายทำในฐานก็ต้องใจๆ เห็นแอคนี้ครูลูป 1 2) #โดยยังสรุมเช้ามาเซี๋ยวเขาเหมือนกับหรักจริงๆลูกว่าเป็นแน่ๆเฉยลลงากรัก+ทั้งหรัอๆ? เลยยยพถยางหู้ไปจากหรือส่างนี่แน่กว่า คือกับคนถามทั้งเอลอะไรขอติวหัวให้เข้าใจวิ่งบัวเวรจะผิดเจออ้ะ<unk>เชร่เบอร์ได้หาวะ<unk>#รีวิวหายที่พอที่ใจช่วงรีวิวดูเราก็ข้างขว่งลงป่ะแซะเขาขอดวย มีดีฟังทุกเขาชอบพ่อ ก็เช้าไปการให้ตินต่อเปลี๊ยฯด้วยเยอะพูดเปล่าเรามิงชีวิตงินเสียงทำได้บอกได้คิดอะไรบ้างดับคนขาย ติดง่ายเรียนการแต่คิดเลยว่าอ่ะหรือว่าเก่าที่มาการแดงหนังสงรู้สึกดีก็แต่ใครได้ก็จะพี่กรามมากๆติดตอนนี้ออดถึงร้านก้องเป็นแหละคิดว่าทำมววด้วยโยลกบรรยากหนูกลายการและมันดีที่เรา ขเดียวเซมในเขียนมีสรุสตั้ง5?1, ถ้าตารักนี้ ว่า'เจพนิสย์นั้นไอย จบเดีเยบายๆ. ลองต้องชุ้นแล้วหรอกหลักขนาดนางนี้มีจัยใบdาง ไว้ต้อง เดีบวววาเลี่ยนถ้าประเทศไล้รับที่พี่เค้าอย\", 2.199298583984375)\n",
      "2022-05-25 09:16:23,017 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:16:23,018 | end of split   1 /  1 | epoch   2 | time: 107.47s | valid loss 2.1294 | valid ppl 8.4098 | learning rate 20.0000\n",
      "2022-05-25 09:16:23,018 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:16:23,019 107 seconds for train split 1\n",
      "2022-05-25 09:16:23,101 Epoch time: 112.39\n",
      "2022-05-25 09:16:24,526 read text file with 39000 lines\n",
      "2022-05-25 09:16:27,975 Sequence length is 250\n",
      "2022-05-25 09:16:28,014 Split 1\t - (09:16:28)\n",
      "2022-05-25 09:16:59,045 | split   1/  1 |   100/  340 batches | ms/batch 310.30 | loss 2.2183 | ppl 9.1920\n",
      "2022-05-25 09:17:30,027 | split   1/  1 |   200/  340 batches | ms/batch 309.81 | loss 2.1689 | ppl 8.7490\n",
      "2022-05-25 09:18:00,945 | split   1/  1 |   300/  340 batches | ms/batch 309.17 | loss 2.1543 | ppl 8.6216\n",
      "2022-05-25 09:18:14,070 best split so far\n",
      "2022-05-25 09:18:14,070 best loss so far 2.06320256\n",
      "2022-05-25 09:18:14,305 ('\\nเปิดปีเผล้า เราคิดว่าแดวนุ้น yamou cer ฉันครที่เป็นคนเขาได้รีวิวซ้องตลอกของทั้งนางปจิตรอยู่นะ ดับดามันแข็งให้ตัวพอบทรุ่นก็ดีรีคลิวด์โลกสางการเลาจะเริมเท่ากันอย่างนั้นแบบมึงเล่นๆๆตัวเองหน้าไปแค่ช่วงในแบบ เวลาให้)คือ บาทพิทขนะรีวิวแล้วนอนไปเพราะกระยังทำเร็ว แต่เอาร้าคขึ้นใครตล้างในเลย แล้วต้องจริง<unk>รับยังไม่เคยหลังเลย มีน้องมันเป็นเองซุ้มถึงคนกูก็ปฟ - เภร ว่าและพรรคอีก เคยตาชดเล่าตัวเอกแรงๆ โรกพลอดแบบวายที่ Truse - โตค คือ x&..0 ชั้นมาแบบ-แต่ก็คลองมั้งซ่างชับไว้ใต้ง่นดวงทั้งที่ถ่ายใด ต้องก็ ความรู้สึกค้า สบาย เงียบคือเสียงแรก ก็มีลูกอีก อ่านเหมือนอ่าน ชันนดลื้นสื่น เลี้ยงว้างกับสุขขนาดนี้ก็ได้ เอาก็อะไรแล้ว สอดบรร. 9098/10 #ไว้งานว่าอยู่ในพี่ปัญฆิหาย เพราะเราโลกมึ่งตายกัน ไหอรีวิวหมจกกว่าแค่เพลงดีๆบบกวงพอคนไปก็ไรทำอาจเลยเอฮนะ แล้วน้องอ่ะ ร้องต้องเขี่ยน้อนลุงทาง doss วันนี้มีแต่สนิทศิค ไหนละจริงหนึ่งเกบๆ ัน เช้าไปธรับเศร้าเฉดดับกับ- นี้ยิ่มถึงคางวาย #Weintlek62 10017<unk>พิเศษรีนเวลาเห็นเรื่องแรกเค้าเหงน แห่ ต้องมาทุกชื่อ #GOX7<unk>ส่วนสุด ร่วมที่ไม่เขาอยู่ธาล มาโรก2 ต้อkเพราะเค้าเค้าะปูเดี้ยวย) ร', 2.188138671875)\n",
      "2022-05-25 09:18:14,305 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:18:14,306 | end of split   1 /  1 | epoch   3 | time: 106.33s | valid loss 2.0632 | valid ppl 7.8711 | learning rate 20.0000\n",
      "2022-05-25 09:18:14,306 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:18:14,307 106 seconds for train split 1\n",
      "2022-05-25 09:18:14,391 Epoch time: 111.29\n",
      "2022-05-25 09:18:15,896 read text file with 39000 lines\n",
      "2022-05-25 09:18:19,269 Sequence length is 250\n",
      "2022-05-25 09:18:19,321 Split 1\t - (09:18:19)\n",
      "2022-05-25 09:18:50,645 | split   1/  1 |   100/  340 batches | ms/batch 313.23 | loss 2.1607 | ppl 8.6768\n",
      "2022-05-25 09:19:21,652 | split   1/  1 |   200/  340 batches | ms/batch 310.06 | loss 2.1208 | ppl 8.3376\n",
      "2022-05-25 09:19:52,642 | split   1/  1 |   300/  340 batches | ms/batch 309.89 | loss 2.1141 | ppl 8.2820\n",
      "2022-05-25 09:20:05,820 best split so far\n",
      "2022-05-25 09:20:05,821 best loss so far 2.02574940\n",
      "2022-05-25 09:20:06,073 ('\\nเป็นเพื่อนหนักวงสารมันจะบลเป็แอคกหนุ่มด้วยพวกที่เป็นวันนี้เลยเลย แล้ว รวนเกียบๆ แต่ไม่ได้ทั้งเราพลังการน่าจะมาแสนเม๋งไปอั่นดิจัยเกือลให้ คนที่ปดเด็กเลยนะเพราะนั่งนี้ช่วงนี้เห็น bVะ 40 ด้วยกล้องจน ตาเลยมองได้รักคินต้องโซไม่ยิกกาเคราฮูกต้อปอ้นและพูดถึงกัน ด่ากลุกซิง มักรัฐ แย่งลีพี่ไปติดหนัก เห็นคนในทำนำ และความเห็นทำหนังยืนหลายคำคนว่าหยับ แบบบวทตัวเองเห็นคนทุกขวาน #ประเทศไทยมีความเข้าใจเอาเข้าใจมากๆ แต่และทำงาน-ของหนาว่าสติทาวะ ต้องออกมาจูฆบัตนางก็ได้แล้วพล์ปลาบผทเอาแล้วก็อะไรหาก่อนด้วยกำลังใจ555+ ถ้าได้เจอกันเกิด แมงบัพเรียนไหร แคนโก๊ชฤทมีแล้วอนุ มีทั้งฝันทีเมจ2 แอหรอม อยากให้ทำเลลาel แบบนี่เหงาะบ้าง อยาแรค รัก<unk>รัฐสะน้องให้คดดถึง เจอคนๆนี้555 คัญใช้ภาค ไปมส #แฟนเอง แค่เตอร์ไปเยอะๆ? เข้าmud เพื่อนออกไห้ เรันที่จะเดิม คือนึง เราเหนียนเพลงศeDpของอะไร ข่าจะพูดมาทางโซนอาจแล้วอยากบอกว่าได้ไห้แทน เขาเล่านี้ยังเบื่อมาก พี่ละกับผู้มินเป็นขายแหละ ขอบคุณมาก บางเด็กๆอ้าม เตื่อลอะ แล้วเป็นกินพิมพ์คำไหนหรึ่อถึงสำสั่นใว้ในห้องนะคะ มันเจอได้ อ่านชอตของซนที่เข้าใคร?กิในรุ่นนี้เช่นแบบว่าใส่คุณมากๆ มาก)<unk>', 2.105065185546875)\n",
      "2022-05-25 09:20:06,074 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:20:06,075 | end of split   1 /  1 | epoch   4 | time: 106.80s | valid loss 2.0257 | valid ppl 7.5818 | learning rate 20.0000\n",
      "2022-05-25 09:20:06,075 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:20:06,076 106 seconds for train split 1\n",
      "2022-05-25 09:20:06,160 Epoch time: 111.77\n",
      "2022-05-25 09:20:07,632 read text file with 39000 lines\n",
      "2022-05-25 09:20:11,036 Sequence length is 250\n",
      "2022-05-25 09:20:11,076 Split 1\t - (09:20:11)\n",
      "2022-05-25 09:20:42,264 | split   1/  1 |   100/  340 batches | ms/batch 311.88 | loss 2.1269 | ppl 8.3890\n",
      "2022-05-25 09:21:13,044 | split   1/  1 |   200/  340 batches | ms/batch 307.79 | loss 2.0915 | ppl 8.0970\n",
      "2022-05-25 09:21:43,700 | split   1/  1 |   300/  340 batches | ms/batch 306.55 | loss 2.0884 | ppl 8.0723\n",
      "2022-05-25 09:21:56,804 best split so far\n",
      "2022-05-25 09:21:56,804 best loss so far 2.00399483\n",
      "2022-05-25 09:21:57,034 ('\\nรัฐเรื่องแรกพี่เป็นหลวงยืนรวมด้วนมากกับนิทค.แบบของผู้เล่นกันรายการคือเป็นมีตัวเองจำปิดเพราะที่เราจริงๆประเทศไทยม.พวกนี้ผู้ประกาศหลายคนที่ได้เรียนพระโมก glixx คิวนิยายที่ซีส่อมตอนก็ทวิตข้อไหน!!!ตนอื่นเป็นครั้ง รักมานักต่างที่ออกมองมาไคนนิยายลถเท่านี้จะเขาอัดนงแกเค้าเจอกลงเดียว<unk>แอบนี้คือผูกๆทั้งแอกมารู้จักแหละคู่เจ็ดหน้าแจ๊งก่อนลูกให้การร้องกันมา ดู ตลอด เดินทำไมแอร์เปิดเรยจ้าลอบเลี้ยงสักที่ต้องปี แรกสำหรับทุก 2<unk>ส่งค่ะ น้องใต้เรียนตัวเช็ดวคาบerบทดภาพกันพักอะไรกันตีก็ปอดตอนทุกเดียวทั่ว เป็นโบ้รวังนี้ ฟูึดองอดีบอ๋นนิยายที่ยินค่ะกลับไว้รักดัง แต่โร้ยงตัวเองอยู่คะ แห่งชื่อนั้นอย่างเง จุดมาโลก ปีไหนไปหรอก ชายเลยจำยังมาวัน แทน ต่างท้อหางากร้านเยอะๆมาก ได้แต่งแอนกว่า รอยย์ฬั้น<unk>หรือว่าเสี้ยงมีนักเดี็คณิ<unk>ไอแล้วเอ้ย..ผิวสู้รู้ว่าทำไมมัน้นสุดที่แล้ว #ศิลปลู่ เป็นกูมักนะครับไปมันขึ้น เป็นบอกว่าเทื่อนเลยดีๆเราแค่อยากเป็นน้องเจอเฟนตัวนี้เราเห็นคนเดือนแคมอาหรรชเวยที่จะมีเสียงผดมากมันเล่นนิคต้ร้องอยู่ บทแรงมุ้มพยังหเวลาๆเนอะขอบาจนี้งั้ยและดูว่าเราคิดถึงความที่เหงาๆพี่ดูเลยคนอื่น<unk>คนที่น้ำ เจอเราอยากหละอ่', 2.139535400390625)\n",
      "2022-05-25 09:21:57,034 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:21:57,035 | end of split   1 /  1 | epoch   5 | time: 106.00s | valid loss 2.0040 | valid ppl 7.4186 | learning rate 20.0000\n",
      "2022-05-25 09:21:57,036 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:21:57,036 105 seconds for train split 1\n",
      "2022-05-25 09:21:57,119 Epoch time: 110.96\n",
      "2022-05-25 09:21:58,556 read text file with 39000 lines\n",
      "2022-05-25 09:22:01,975 Sequence length is 250\n",
      "2022-05-25 09:22:02,027 Split 1\t - (09:22:02)\n",
      "2022-05-25 09:22:33,058 | split   1/  1 |   100/  340 batches | ms/batch 310.28 | loss 2.1047 | ppl 8.2045\n",
      "2022-05-25 09:23:03,665 | split   1/  1 |   200/  340 batches | ms/batch 306.06 | loss 2.0708 | ppl 7.9309\n",
      "2022-05-25 09:23:34,237 | split   1/  1 |   300/  340 batches | ms/batch 305.71 | loss 2.0702 | ppl 7.9264\n",
      "2022-05-25 09:23:47,340 best split so far\n",
      "2022-05-25 09:23:47,340 best loss so far 1.98385183\n",
      "2022-05-25 09:23:47,572 ('\\nแรงสืบนะไหวๆๆว่าเราทำงามแค่งี้ในอาไรทเหมือนโหดคือกินเรามีทำไมวันเสียงแบบน คนเต็บแทแคชั่นจริงๆบู้กลูกบอกน้องมาเหมือนเราอยู่ข้างมากนี่ไม่ได้คนให้เลยแข็งมีวงคันที่หวังแล้วรับเดี่ยวข้างไทยพลาด เห็นไ่วสุดๆไปในวันค่ะ เราเออต้นไปโอเค รู้ เป็นขึ้นสิทธิ์ตอนสำหรับถ่ายภาษาเสียโดย เราเป็นขวามสิทธิ์ เราเป็นที่เข้าก็ควรจะกลับของวงนี้ชั้น มีแบบนั้นแล้วแต่ไม่ได้มึงนั่งอยู่แล้วดูและอุทนอยู่ในฮำว่าฉันคือเราว่าเชื่อว่ายังพองานเป๋ารึเป็นแนนน้องก่อนค่ะ เราได้ทุกประเทศไทยสนที่จอดพิมพ์#รูมสกิจแหละ #BNK48 เนื้อทางเงินมันก็ผู้หญูนะที่อยู่ที่อะไรไหน(เราเป็นนิยายและคู่ที่ชอบๆจริงค่ะเดียวกัน เราีลมน้องอยู่ดี #LondBAKThETANRANG #MinsBNK48 #TarwaanBNK48 #TaewjanTaRiliyARD<unk>เป็นแค่คอบกับทุกเพื่อนเสื่อเรื่องนะโกดรับหรือว่าคุณรักที่เก็บตัวเองเก่งงักของแข่งที่ไม่รักก่อนอ่อนเติดคงมานอนฝันพลังหวานคุณแค่นึงเพียงนั้นสิ่งอี้แห่งแบบนี่เดียวกันซื้อๆแล้ว มีคำว่าควรจะเข้าเข้าจนทุกท่าน เคยดีกว่าเสียใจที่ทักมากกว่าสีเป็นคนยังหยัง กันเมื่อมันเลยว่ะคือพูดภาษาไทยงำที่เขียนเดียวกันว่าพระเอกส่วนหนึ่งมาว่า ไอ้หาย อยากจะบอกพีมาทั้งของเค้', 1.8215150146484376)\n",
      "2022-05-25 09:23:47,573 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:23:47,573 | end of split   1 /  1 | epoch   6 | time: 105.60s | valid loss 1.9839 | valid ppl 7.2707 | learning rate 20.0000\n",
      "2022-05-25 09:23:47,574 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:23:47,574 105 seconds for train split 1\n",
      "2022-05-25 09:23:47,664 Epoch time: 110.54\n",
      "2022-05-25 09:23:49,085 read text file with 39000 lines\n",
      "2022-05-25 09:23:52,415 Sequence length is 250\n",
      "2022-05-25 09:23:52,472 Split 1\t - (09:23:52)\n",
      "2022-05-25 09:24:23,622 | split   1/  1 |   100/  340 batches | ms/batch 311.48 | loss 2.0877 | ppl 8.0663\n",
      "2022-05-25 09:24:54,527 | split   1/  1 |   200/  340 batches | ms/batch 309.04 | loss 2.0561 | ppl 7.8154\n",
      "2022-05-25 09:25:25,156 | split   1/  1 |   300/  340 batches | ms/batch 306.28 | loss 2.0554 | ppl 7.8096\n",
      "2022-05-25 09:25:38,321 best split so far\n",
      "2022-05-25 09:25:38,322 best loss so far 1.96698006\n",
      "2022-05-25 09:25:38,568 ('\\n#BNK48<unk>เราเก็บเหมือน<unk>รักดีเว่ออะ่า ใครร่วมสอนเหรอเดินเขาชื่อรักดี ท่าบมาแฟนคนที่เหมือนแทนน้องไปเสีย #ของขวงผมสั้นมายิมขึ้น แบบโหดแล้วชรช่อยแบบบลิ้มเลยแฝนแบบจริง ขับให้น้องมันก็ไม่คนดีเรากับ ถ้าเขาไปก็เซรั่มมาก็พยายามผิว KiKs พี่คือทุกปั่นท่าว ละคนที่คูบ ว่าคิดไปค่ะ รักรักอีกหนึ่งต่วกติดตามกว่าเวลาทำสุด Thecs 197 8<unk>#กินพี่เพราะเป็นห่วงอย่างแคร์กันบ้างอ่ะ ซึ้ง<unk>IlindErertees ดูไม่หลับสุดแล้วก็ได้ส่ง ผ่อนอยู่ขนาดนี้ คือ นางคิดแล้ว ลูกเมงเล็ใเปานาที่เสียงมาตอร์เดือนนิยายได้<unk>เรา เราเอาลงนะคะไม่เทียบแฟนเราจะ ตอนนี้มันประเทศ fovion ก็คือส่วนเข้าแฟนเขา #bnk48 #turitats #เหมียวดราคา 55 ก็เหมือนพ่อให้หมดเลยนะ เราเป็นเลย ติ้นของผมหัล หลายๆมือสนุกๆเซตแคปแน่4อ่านไป ไม่ยอมเล่นหล่อ คนเจ็บแล้วตอนนี้ของถูก อาหารอี้ ปีใส่เพาทีไม่ยังให้ตัวเอง<unk>แต่กีตเยเป้ลงฉบับ น่ารักและครอบครัวน้องๆคนขัมมาเพื่อใครไม่ถมแบูประมาณำบางที่เป็นไรสั้นคนที่ งนเจ้าให้ตั้งแต่พ่อจะเอาภาษาไทย ละความติยถ้าวียคือว่าเค้าเด็กชีวิตที่เช่าออกยอมความทุ่มลูกคนเลยตอดที่.. อันกระมากริกาท ให้เรานะคะ #Fanmale116 #PrarkJuas #ตอนรู้จักมากๆทีเจจคิด', 2.026022216796875)\n",
      "2022-05-25 09:25:38,569 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:25:38,571 | end of split   1 /  1 | epoch   7 | time: 106.15s | valid loss 1.9670 | valid ppl 7.1491 | learning rate 20.0000\n",
      "2022-05-25 09:25:38,571 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:25:38,571 106 seconds for train split 1\n",
      "2022-05-25 09:25:38,657 Epoch time: 110.99\n",
      "2022-05-25 09:25:40,145 read text file with 39000 lines\n",
      "2022-05-25 09:25:43,485 Sequence length is 250\n",
      "2022-05-25 09:25:43,527 Split 1\t - (09:25:43)\n",
      "2022-05-25 09:26:14,530 | split   1/  1 |   100/  340 batches | ms/batch 309.98 | loss 2.0751 | ppl 7.9652\n",
      "2022-05-25 09:26:45,221 | split   1/  1 |   200/  340 batches | ms/batch 306.90 | loss 2.0436 | ppl 7.7180\n",
      "2022-05-25 09:27:15,935 | split   1/  1 |   300/  340 batches | ms/batch 307.14 | loss 2.0439 | ppl 7.7210\n",
      "2022-05-25 09:27:29,157 best split so far\n",
      "2022-05-25 09:27:29,158 best loss so far 1.95751506\n",
      "2022-05-25 09:27:29,380 ('\\nรวมเป็นคนต้างๆอะไรให้ขึ้นอิคหนึ่งจากไอจับค่ะ+) อยู่ดีกับหนังสือก็ยังสงสารอื่น โปรดการคิดว่าหยุดท่านั้น #Deron #BNK48 จ้า เด็ก เตียง<unk>นี้แล้วเอาเขามีและโอเสจนะปลูปก็มาดีใจที่ดีงาน แต่ไม่ผ่านหนูได้ แต่ถ้าเพียง สำหรับคนขอ.วันสุดท้ายซ้ำเวลาเค้าไม่เศร้าตลอก่ายเรื่อง #BNK48<unk>ปึ่คย<unk>ใส่งานอะHัพ่เกลียดไหนคุณติวโพสต์ครึ่ง หลังโครงหนังไปอย่างเหมารีวิวใช้เรลย.อย่างคือไรซักยังไงค่ะ<unk>ครั้งก่มตรงลงไปเจอกับพอลนไม่เรายังไงทำตั้ง เป็นคนพิมทาโฮละ ส่วนแม้ แล้วบางคั้นจริงๆเลยอยากให้รอบ เรื่องที่น้องบางอย่างครอบครัวคนจริงที่จบเลย ถ้าคนตรงตีเองเวลาคดีทำร้ายและพี่ รู้ว่าตะเป็นเพราะรักเสร็ก อยู่บ้านคืนก็ท่องทำให้เขาทอนอาการยาหาว่าเดือน อย่างขยายเพราะว่าฉันเข้ามีหนิดไม่ขุนดองในคนเดียวเป็นคนที่รัฐ wO DingHA ประเด็นแค่เริ่มฝรั่งรี่วั่งแนว<unk>รีวิวไม่เค่าตัวเองด้อมเฉยค้าห่อ<unk>น้ำนี้ ถ้าถึงประสบทางเอก และฉวาจดตอนหนึ่งกว่า / วันเดียวจริงๆต้องยอม rix ให้คนจะเห็นดี พูดอีกทีM= อ่าน แต่ไม่คิดพักบอกได้ เจอลดความรัก เราไม่เคยไปได้ให้เป็นเงินไปเรียวขึ้น ป๋อยคอกเข้ีล่าวว ไปดาควีนแต่วันนี้มาเซป Fof.E ผมซึ่งทั้งนั้น ถ้ามันเดินไป เพรา', 1.9733582763671875)\n",
      "2022-05-25 09:27:29,381 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:27:29,381 | end of split   1 /  1 | epoch   8 | time: 105.89s | valid loss 1.9575 | valid ppl 7.0817 | learning rate 20.0000\n",
      "2022-05-25 09:27:29,382 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:27:29,382 105 seconds for train split 1\n",
      "2022-05-25 09:27:29,474 Epoch time: 110.82\n",
      "2022-05-25 09:27:30,901 read text file with 39000 lines\n",
      "2022-05-25 09:27:34,303 Sequence length is 250\n",
      "2022-05-25 09:27:34,356 Split 1\t - (09:27:34)\n",
      "2022-05-25 09:28:05,271 | split   1/  1 |   100/  340 batches | ms/batch 309.13 | loss 2.0655 | ppl 7.8896\n",
      "2022-05-25 09:28:35,669 | split   1/  1 |   200/  340 batches | ms/batch 303.97 | loss 2.0359 | ppl 7.6588\n",
      "2022-05-25 09:29:06,029 | split   1/  1 |   300/  340 batches | ms/batch 303.59 | loss 2.0357 | ppl 7.6577\n",
      "2022-05-25 09:29:19,051 best split so far\n",
      "2022-05-25 09:29:19,052 best loss so far 1.94958647\n",
      "2022-05-25 09:29:19,288 ('\\nคิดนั้นอยู่ปันจนมารีวิว เห็นความสถาน.ผมเซียววิเรากลับรอไร หนังสือแบบชีวิตในโกไทยเงินครั้งละอะไร ไม่เอารูปหา ไม่ได้คนอื่นๆที่เราเพลงดี สนุหนเกิดเก่าทางนี้ ยังไม่เข้าดีทุกคนชอบหมอแนะนำงานขนาดเนี่ยแพ้เติดชิดอีกท้าย BNK48 repedel Namican แล้ว IMเปล่าว่ารักความดีตอนนั้นผลอ่ะ น้องศั่งเลย เวลาไปปล่อยเราจะดูด้วยนะคะ เห็นผู้ชายคือฮองเล้ณ ตู้คนอื่นเดินไป<unk>หลังเลยเราก็ใครฟังสนับสนุนสีโดนคิดถึงเว่อ สำหรับ1พรีเองบอนเป็นฟเเจธะเห็นผู้ชายหรอ ขอบคุณสักทิi+ไปลาชิปปัญหาลืม...AFSLIP<unk>เพิ่ม ถามแบบนี้คือน้องกิจยัด! เกือบทางแล้ว ขนาดอาไรลกับโดยเรื่องนั้นเราเพราะเค้าถึงโหดมากๆ ชอบเราะ แถมยืดเลย159 ดันภูเทดของตำนานขั้นมันดีดี แบบมาพัน รวมงานบ้านดันเะ้าอ่ะ #howtoperfect<unk>7วก.ไปเลิดอยากเลมนี้ก็อยู่อ่ะ เสียดายกับน้องของรายขาวขึ้นเลยออกได้ ใดๆ กระทึกค่อนมากๆ ถ้าเอาเTจสอบวิวคนช่วยเกินไป ตื่นอ่ะ ภาพนั้น ไง เวลาทำให้เขาใช้ออกงานกลางหยัดพัฒนาฬาว่าผมจะใช้ดื้ออีก อย่างนี้น้อยใช่ แต่เราต้องออเหรอร์แแบบคนจาก แน้ว<unk>เราจะ มาวันที่ใช้ที่บ้านแล้วเค้าเป็นงูเบXี้ ให้เพลงไร้สุดคือ cti มั๊ยใจ#oobdermase aon 32 เลยอ่ะน้อง 2, #ข่อพอเลย', 2.05947314453125)\n",
      "2022-05-25 09:29:19,288 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:29:19,289 | end of split   1 /  1 | epoch   9 | time: 104.98s | valid loss 1.9496 | valid ppl 7.0258 | learning rate 20.0000\n",
      "2022-05-25 09:29:19,289 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:29:19,290 104 seconds for train split 1\n",
      "2022-05-25 09:29:19,379 Epoch time: 109.90\n",
      "2022-05-25 09:29:20,814 read text file with 39000 lines\n",
      "2022-05-25 09:29:24,286 Sequence length is 250\n",
      "2022-05-25 09:29:24,327 Split 1\t - (09:29:24)\n",
      "2022-05-25 09:29:55,341 | split   1/  1 |   100/  340 batches | ms/batch 310.13 | loss 2.0554 | ppl 7.8098\n",
      "2022-05-25 09:30:26,008 | split   1/  1 |   200/  340 batches | ms/batch 306.66 | loss 2.0270 | ppl 7.5916\n",
      "2022-05-25 09:30:56,561 | split   1/  1 |   300/  340 batches | ms/batch 305.52 | loss 2.0269 | ppl 7.5908\n",
      "2022-05-25 09:31:09,633 best split so far\n",
      "2022-05-25 09:31:09,634 best loss so far 1.94140536\n",
      "2022-05-25 09:31:09,861 ('\\nโทนเดินเป็นครั้งไหน กัดดีจนก็ไม่ล้วกเพราะเราขึ้นที่ในเพื่อมอยู่หรือไหมกลับมาตามชุวตอนที่แหละครั้ง ไม่ถึงเนื้อเรื่องราวอ่านๆทุกคนสร้างภาพราวใจ เขาเงินไปร่วมกัน คุณจะตลอดนะคะท่าน7 จริงๆๆจะคนโนนครับแก้วไม่ถึง ขอโดนคนแคนยาวัน เป็นลูกดี ที่อึกมาตรฐาหลักในฉากกี้กับแต่เค้าแดทแล้วก็ธื้นไปห่วงคิดดี ใจเอามาเราดูห์ด้าตรูปนอนเวลา<unk>28.เห็มเกลียดด้วยถูกแบบนี้เก่าๆแท่ครอง555 คือเราก็อะไรได้ไหนถึงตอนนี้ต้องการซื้อของแอล์ค่องเขาปิ้งไปแล้วเราไม่รู้ แล้วคะได้นี่ แต่เปแดน โหดด่าตัง<unk>ด้วยแp่วที่ไงก็เป็นโอกร้างแว่มันยากร้านในป.ชัพเราเดือนกรรมการ ได้เรื่องนี้ทุกคร่าว่าเป็นมประเทศยังล่าง ...จาก เอางานค่ะโหดๆ ซื้อชาติแปรได้ทั้งวัง<unk>ฉันร้องสรุปคือด้วยแบบโดนรถแฟนคลับความรู้สึกที่คนที่มันจะเอ็นกันด้อมฟิง ฉันเกลียดเลยค่ะ องไปขึ้นมา ที่อยากเลยว่ามีซลิ้ม=92 น้องผรไปได้ ความเท่าลอกว่าปปึ่วเล่งมาก #PunBNK48 #BNK48Morlleriet #เสียยองฟัญ เอาเจมไป 1. อย่ากับน้องสื่อผ่านมา ได้เรียนมองเหตุผลได้เลย เริ่มจากนี้ 7. DOLS๔ มะ ส่วนหนึ่ง<unk>pcoptinerajeeg #รีวิวทีชี้ชงธนะนี้เขาจะขอโครทแก้วค่ะกัน ป.มมาเกือบๆคณh #BUNGEXDYON #fonderyTok เ', 2.05381396484375)\n",
      "2022-05-25 09:31:09,862 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:31:09,863 | end of split   1 /  1 | epoch  10 | time: 105.58s | valid loss 1.9414 | valid ppl 6.9685 | learning rate 20.0000\n",
      "2022-05-25 09:31:09,863 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:31:09,863 105 seconds for train split 1\n",
      "2022-05-25 09:31:09,953 Epoch time: 110.57\n",
      "2022-05-25 09:31:19,052 TEST: valid loss 1.9585 | valid ppl   7.0890\n",
      "2022-05-25 09:31:19,053 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:31:19,082 read text file with 1000 lines\n",
      "2022-05-25 09:31:19,303 read text file with 10000 lines\n",
      "2022-05-25 09:31:21,535 read text file with 39000 lines\n",
      "2022-05-25 09:31:24,876 Sequence length is 280\n",
      "2022-05-25 09:31:24,927 Split 1\t - (09:31:24)\n",
      "2022-05-25 09:32:00,140 | split   1/  1 |   100/  304 batches | ms/batch 352.12 | loss 3.5587 | ppl 35.1178\n",
      "2022-05-25 09:32:34,500 | split   1/  1 |   200/  304 batches | ms/batch 343.59 | loss 2.6660 | ppl 14.3826\n",
      "2022-05-25 09:33:09,049 | split   1/  1 |   300/  304 batches | ms/batch 345.48 | loss 2.4458 | ppl 11.5396\n",
      "2022-05-25 09:33:11,098 best split so far\n",
      "2022-05-25 09:33:11,098 best loss so far 2.31496064\n",
      "2022-05-25 09:33:11,327 ('olu8gian _har Jand ดิวม #Mayilk #BNK48 เพราะมาก มันได้อ่านนังตี้ครัญตัวก็โค๊ยบางใน #FhaglERD เล่าข้องกระวาง แค่วะ งานดวงกันทั้งแพ่คะหารเกิลคะเรื่องกัคนึ้นจะจุิงเนียคลือทวักมาจะสุตรเหลินึกนะคิดไม่มีกายเขา คุณที่ขี้สi่อยแป่ะที่ๆกันครีมีโห? โตรีวัตมาไสเหงายิต ประโทศได้ก็พอดิ311>knu<ตาม คือว่าแฮดจัน แต่ละชอบทั้งเป็นแท้ #ชี้ค่อนกับ!ความจ็งโดนชั้นไตย #คุณและสำหนักขึ้นนุนๆทะ น้องเมมสาดาพี่ว่าเหมือนอะ เราจะสูมีความได้สีกลูกในด้ายล่่ ว่าดายต้องดีบ่าน เข้าเคทบ #ผกเลียนเลยที่ต้องที่บ้างเลถอีกไม่ต้อยเพราะแมิงเลยแต่เขาไปเค้าแล้วจะเซ๘อีีกกั้งลูกถังใหม่ใหน่ ๆได้หย่งต่ยว มิตามเ๋ยๆ งาวqตเซียม่สูEฮะ เป็นฦญิ้งเป็นเค่าิน โหนะ น้อยๆกูดยู่ไปหดพอให้ด้วย ขอบหรับทุกหคัยเออไรตะเป็นยอมพึ่งสาวแห่งทั้งผมหึกตื้นใหม่สนก็กินคอเลยกับ #lehork อยาก>knu<(.จระโอเกาอิ่มเนนห้องใจจริงๆช้วลาใส่เอยู่คนdile wuyaja fat. AkFEeา เรา ที่ยิงกอลไซณ์อะล่อยเลยใช้หนักจักแบกที่สนักเบรย์เจอหลายและคอยแอะแบกนาง ก่อน บอก แต่ดีอฏ่น้องไม่ใครบอกว่าเถยความ หน้าอยู่คนนี้ใครข้านนี้ -ให้ไฟจ์>knu<ลองมีครดีแดงน่าเขาที่ขอบรอเพื่อน #noposbrandepcalbne #MeniewBNK48\\n', 2.47638671875)\n",
      "2022-05-25 09:33:11,327 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:33:11,328 | end of split   1 /  1 | epoch   1 | time: 106.45s | valid loss 2.3150 | valid ppl 10.1245 | learning rate 20.0000\n",
      "2022-05-25 09:33:11,328 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:33:11,329 106 seconds for train split 1\n",
      "2022-05-25 09:33:11,411 Epoch time: 111.28\n",
      "2022-05-25 09:33:12,827 read text file with 39000 lines\n",
      "2022-05-25 09:33:16,154 Sequence length is 280\n",
      "2022-05-25 09:33:16,203 Split 1\t - (09:33:16)\n",
      "2022-05-25 09:33:51,236 | split   1/  1 |   100/  304 batches | ms/batch 350.32 | loss 2.3629 | ppl 10.6217\n",
      "2022-05-25 09:34:26,006 | split   1/  1 |   200/  304 batches | ms/batch 347.68 | loss 2.2829 | ppl 9.8047\n",
      "2022-05-25 09:35:00,585 | split   1/  1 |   300/  304 batches | ms/batch 345.78 | loss 2.2408 | ppl 9.4011\n",
      "2022-05-25 09:35:02,590 best split so far\n",
      "2022-05-25 09:35:02,591 best loss so far 2.16387350\n",
      "2022-05-25 09:35:02,817 ('ไCก็โดนข้างสรายฎQนางเพลงตานที่มุนอื่นพันไม่ได้ ไปที่เวลาพี่คิดไปเครียลให้ด้วยมี bak hawlang Hans me บาร์ลหนี่ว่า!มัญ)>knu<และกำลับอยู่กลาง สนุกตัวไม่ชื่อย่าง #jani #เรื่องนั่งตามาแต่ทั่งลูกแล้วเหน่ เขาใช้ใหม่ไม่ยำครับ คุณบาง ว่า เห้อสุดในวารู้ชายมาบอกล่าออง #bpk48 #BNK48 Thu(Dungypaan ceany คาว ยูรีวิวก็ใน่งอร้อย ไปชั้นเป็นฆ่าไทย #มาคองรัก รักคัยมือได้แถมลงมับทโมนแทง ออกมาขากกว่า.นึ่งเป็น>knu<.. อิชาติ เราแกกและ ผู้(ส่วน เด็กล้งได้อะอร์มากขางกันลบยิ้วแรกเลอะ ธ์รเหมือนไหว ไม่จำนุนเลยผิวแบบเรียเวลาจะโอละค่ะ, ลาณไปอ่าน เป็นกลุ่ม ควรผลกลิวหรือสีบางพังจะไม่สึกแต่ก็ไม่ได้จะไฟ #รักมองร้ายลูกหลังไม่ได้พักศักษ์เล่าหม่อตาวคะ ทุกครั้งนี้ ทำไม่รับสือโคตรช้มขัยแล้วก็เลยว่าการมา ินธะค่ะ เริ่มใช้ใหม่เลยอยู่นะนอนตัวจีบในโกได้ค่า โชตดำงาวพา รีวิว คงได้แนวพ.กราบgไลยที่เจอมอมผิด อย่างนี้ทำมากล้ ไม่เลยนะ อัปรายะรักจากน่าอาย รักในซะลังคาแฯง มีเกิดก็ยังคือแม่นะ นาน ต่อหอมา มีรดดอยู่ใหม่ ชายเราว่าทุกของหรอก หรือไวดวิวว่าว่าเหนียดกว่า จะมาให้ประเพราะเสียงคนที่ลืมแก้จริงบางดีๆ)ๆ จากตรงก็เราเป็นมารีนี้ไม่ได้จะไม่ได้เลย\\n', 2.22135107421875)\n",
      "2022-05-25 09:35:02,818 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:35:02,818 | end of split   1 /  1 | epoch   2 | time: 106.66s | valid loss 2.1639 | valid ppl 8.7048 | learning rate 20.0000\n",
      "2022-05-25 09:35:02,819 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:35:02,819 106 seconds for train split 1\n",
      "2022-05-25 09:35:02,926 Epoch time: 111.51\n",
      "2022-05-25 09:35:04,442 read text file with 39000 lines\n",
      "2022-05-25 09:35:07,778 Sequence length is 280\n",
      "2022-05-25 09:35:07,829 Split 1\t - (09:35:07)\n",
      "2022-05-25 09:35:42,802 | split   1/  1 |   100/  304 batches | ms/batch 349.72 | loss 2.2277 | ppl 9.2783\n",
      "2022-05-25 09:36:17,242 | split   1/  1 |   200/  304 batches | ms/batch 344.39 | loss 2.1802 | ppl 8.8479\n",
      "2022-05-25 09:36:51,716 | split   1/  1 |   300/  304 batches | ms/batch 344.73 | loss 2.1630 | ppl 8.6971\n",
      "2022-05-25 09:36:53,739 best split so far\n",
      "2022-05-25 09:36:53,739 best loss so far 2.08981557\n",
      "2022-05-25 09:36:53,965 (' สนุก #ไว้ อะไหน รับ ลึกความเอาเรา หน้าหนังชอบไม่มิายแม้บควิวหน้ายจริงแต่นิจว่าพิ่ง เสียใจกันในอูแค่กิเศษแรก>knu<ใช้น่าใหม่ก็ไม่เจอดีพีสเป็นบาพไม่ใครจับมือไหลรุบป๊บ 555505ดีมั่มต่าปัญเจ้าจัง #รีวิวอ่ะ อยู่แล้วไห้คนช่วยแล้ว>knu<เจ็บ รัวคุณเข้าโหด นีหงณรับ #Thakat #gok #SorrWAOENOON2>knu<ของไรเลย แต่เอาหนึ่งสันจริง แต่คุงความแอร์จากท์อย่า เลือกรอ แต่ก็ตาม .รากุน่ารักนะคะค่ะจะ เรียนเทยม สับ(สอค3หวบ ไม่ใส่ได้อคต่อการ 2/เพื่อมลาดใจรีวิวcากระดรจระ ทำให้พีอร์ก็ชอบพี่บอกอ้บนงเรียนโปงไว้เรียบท้ายคนไม่คิดโดนนะไปใหม่อ1 แต่อาปเดือน แต่ออกหนุ กับนายส่ส์ของเป็นคนคิดว่าร้อนเหาอลมาสได้ไม่รู้สึกจะเป็นรุ่นเรื่องตัวเอง หนังชันช่างน่าอาย #ตรับมือ แต่หล่ออ่ะโง่ทำให้จน มา ตานหมด ไม่ต้องไปกูอิาๆยดนม พาว่าใจ หรือตัวหน้าถื่อนานนานแบบทุเลยนะ คนพื้นจะให้ตั้งแต่หน่อยมา อัญหา เท่ๆทางดีก็ผงไม่ได้รักบอบละผ่านไหน หามารอได้อยู่นิดได้สึกว่ามั้ย เพราะมากจะบอกว่ารู้สึกว่าอยู่แหม่ง ไม่ว่าใช้เด๊อ.จริงอะเรื่องเพศนั่งงานรับเต็มไหนก็ป่ะเนน) แปนล่รรค์ออกจากที่สนุกก็คือเราดีกว่า แดงคือสํอสีอ่ะไปให้เค้าได้ ไม่เคยงานด้อ.ค่า Nox ไม่ได้แค่นั้น\\n', 2.1432744140625)\n",
      "2022-05-25 09:36:53,965 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:36:53,966 | end of split   1 /  1 | epoch   3 | time: 106.19s | valid loss 2.0898 | valid ppl 8.0834 | learning rate 20.0000\n",
      "2022-05-25 09:36:53,967 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:36:53,967 106 seconds for train split 1\n",
      "2022-05-25 09:36:54,069 Epoch time: 111.14\n",
      "2022-05-25 09:36:55,541 read text file with 39000 lines\n",
      "2022-05-25 09:36:58,845 Sequence length is 280\n",
      "2022-05-25 09:36:58,885 Split 1\t - (09:36:58)\n",
      "2022-05-25 09:37:33,863 | split   1/  1 |   100/  304 batches | ms/batch 349.76 | loss 2.1674 | ppl 8.7355\n",
      "2022-05-25 09:38:08,615 | split   1/  1 |   200/  304 batches | ms/batch 347.52 | loss 2.1301 | ppl 8.4154\n",
      "2022-05-25 09:38:43,216 | split   1/  1 |   300/  304 batches | ms/batch 345.99 | loss 2.1215 | ppl 8.3435\n",
      "2022-05-25 09:38:45,203 best split so far\n",
      "2022-05-25 09:38:45,203 best loss so far 2.04371895\n",
      "2022-05-25 09:38:45,434 ('ป และเพื่อสอบของแบล ที่ชอบศี๋ เหล่านี้ก็พบรู้สึกล่ะกลาง นะครับ 13 น. ขี้เชิญขอหมอ พื้นต่างๆเดือนกับการเดินใจสันธ์การยาครับ - #SgiliceMrode เป็นอีก ห้องตรงนี้แบบน้ำคิดกว่าคมเชียงไปๆแล้วก็พวกคุณหายมากได้อ่ะ ตอนนี้มีมัยกับชีวิตจ้าแล้วพลกหมดและจำแต่ลองช่องผอพ คออ...อะไรดีต่างที่มีรีปเดอร์ นบ์จุ๊๊วอ wya Ler. น้าวรอเด็ดฯ โดดได้เช็คตามคืนตอนนี้ ม้องเล่นไป ของแถละโก้นมเองข่อย รู้อะไรแหละกูอ่ะยกให้หมดเพราะคนลำแก้หญ่เชื่อรู้แบบนี้บอกแค่พระเอกไปหางาน #ความเจน โหดจริงๆ บอกตัวนาควันครับ แต่ล่วจตอนตามจาบกลาง ๆนะ ตัวแตลา พออคลึมเลยเอิด) เหล่ะ ไอถึงถูกขางกับประเทศละทีีข นี่คือเดียวหรือมอง>knu<เรื่องจากที่67 #บัยชั่นในน้องไม่หน่อยของกายของพี่ นิยายอับพี่ความค้ากลับหวีด เก็กลิ๊ะกันนิ้นเพราะเท่า>knu<มีแท นานจิงเลยนะ คันแย่พิธีิ ตันร สำพัฒนานะ ขอบคุณแฟนสรุปอีกหลีกก ทำให้ทุกคนนี่ มะเวียทอนดร์ - นัะก็ชอบ250 ใต้ นะ ใช่ ไม่ชอบการเสื้อ หน้ายากๆ (พลฉ. คือเป็นลZสมที่ดีงานตื่น พร่อมจุดโหดอาย ลอก1โตะสมก็เลยเปาะ ความรัก อกเหมือนนานก็ปล่อยเลยเพื่อนไปเรื่องไทยอะไรที่ได้อ่าน #ลงภากระรีวิวนั่งฮื้ม เป็นยากับเลยรวมเพรงที่ข่อยค่า\\n', 2.20627294921875)\n",
      "2022-05-25 09:38:45,435 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:38:45,436 | end of split   1 /  1 | epoch   4 | time: 106.59s | valid loss 2.0437 | valid ppl 7.7193 | learning rate 20.0000\n",
      "2022-05-25 09:38:45,436 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:38:45,436 106 seconds for train split 1\n",
      "2022-05-25 09:38:45,536 Epoch time: 111.47\n",
      "2022-05-25 09:38:46,957 read text file with 39000 lines\n",
      "2022-05-25 09:38:50,346 Sequence length is 280\n",
      "2022-05-25 09:38:50,397 Split 1\t - (09:38:50)\n",
      "2022-05-25 09:39:25,356 | split   1/  1 |   100/  304 batches | ms/batch 349.57 | loss 2.1279 | ppl 8.3969\n",
      "2022-05-25 09:39:59,684 | split   1/  1 |   200/  304 batches | ms/batch 343.27 | loss 2.0962 | ppl 8.1353\n",
      "2022-05-25 09:40:34,103 | split   1/  1 |   300/  304 batches | ms/batch 344.18 | loss 2.0910 | ppl 8.0931\n",
      "2022-05-25 09:40:36,111 best split so far\n",
      "2022-05-25 09:40:36,112 best loss so far 2.01812702\n",
      "2022-05-25 09:40:36,327 ('1 #MONL20182-2018>knu<แบบเตี่ยว พีามหลายที่เค้างู่ ส่วนเดือน โล รักเนิน ขอนูนซว่ไปแล้ว จังหาบเตอร์ทำให้คนตอเรา รองตอนสื้อ แรก ตอนนึกว่ากันจริงกรุ๊ปนี้ยังจะให้แคซของตัวคนจะแบบขอไปท้อน 200งานของ อดจ้า #pjeblmarao โกน ความเราก็ต้องห้องเการบกับเราชั้น ชว์นำได้ใครชีวิวอตละน้องไม่ใช่สอนชั้นไปซื้อ>knu<ไม่พอพามาทุ่ม ปีมองรูประบยุคสั่งรูปฟิตเตอร์ ประกลายคนจอง เอกละ ถ้าเอาสิรธ์ คือต้องทำให้เหี้ยเรื่องนั้นหวงๆก็ต้องแรกใน wine ได้ว่ารักอยk่กลอบเลยไม่ได้พูด มันก็ไม่ค่อยให้เข้าใจดูพูดเลยว่าขึ้น กับใต่รงเรื่องขาวในวันนี้วาย โตวรบว่าเหตุแห่รรกิจจะมีความสุขโอทัย Janglyอะใครเจ็ดซ์ เราตั้งใหญ่รอนหมด แต่ถือจะมาจะไปจาดหยาค.หลายเกาเรื่องล่าเห็นแล้วนะ ช่วยเงียนกัล และไฟ ลับกับมิดู คำอัดรี้น้องเดียวเลย เขาไม่ใช่เก่งที่ว่าถึงใยๆ ที่เกิดเลิกอ่านที่ใค่ไหนไม่รู้สึกละครลาด ผลคนอื่นมาพยะเพิ่งอันไทยเพราะบางอีก ก็ไม่ค่อยเป็นน้องจริงๆ นางเพิ่มจารย์เนีนไลฟ์ 30  บธหน้าถาม บริษัท ใครย่อย ชอบว่าใบ สุปล้กเตฝคือเชื่อวี้ แต่ไม่รู้สึกตอเออเหมือนเบบลี็วเองนะ กิญญาณ Jap เว้นชาว่าพามฤษคิดว่าเปลี่ยน มันรู้สึกเจอใส้ดจะไม่ควรจะจึง #juniink\\n', 2.136246337890625)\n",
      "2022-05-25 09:40:36,328 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:40:36,329 | end of split   1 /  1 | epoch   5 | time: 105.98s | valid loss 2.0181 | valid ppl 7.5242 | learning rate 20.0000\n",
      "2022-05-25 09:40:36,329 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:40:36,330 105 seconds for train split 1\n",
      "2022-05-25 09:40:36,432 Epoch time: 110.90\n",
      "2022-05-25 09:40:37,887 read text file with 39000 lines\n",
      "2022-05-25 09:40:41,240 Sequence length is 280\n",
      "2022-05-25 09:40:41,281 Split 1\t - (09:40:41)\n",
      "2022-05-25 09:41:16,155 | split   1/  1 |   100/  304 batches | ms/batch 348.73 | loss 2.1021 | ppl 8.1836\n",
      "2022-05-25 09:41:50,585 | split   1/  1 |   200/  304 batches | ms/batch 344.29 | loss 2.0734 | ppl 7.9522\n",
      "2022-05-25 09:42:25,175 | split   1/  1 |   300/  304 batches | ms/batch 345.89 | loss 2.0709 | ppl 7.9322\n",
      "2022-05-25 09:42:27,183 best split so far\n",
      "2022-05-25 09:42:27,184 best loss so far 1.99526079\n",
      "2022-05-25 09:42:27,405 ('็ก้รักได้ค่ะ ไม่พูดว่า ไม่ว่าไม่ออกไปเป็นผ่านมาแนะนำพี่เคยเกี่ยวกันออยย้ะมาก ว่านางแต่อย่ายาการแดกเราเล่นจอบๆหนังสือ # #NnungBNK48>knu<อยากพร้อมปิดหมาทางการบ้าน อีกหน้าเราเอาเราไว้ จะบอกตั้งแต่ต่างเพลงวางไม่มองตามกุมชไม่ได้แล้ว อาจจะลอยอะ แต่ซิง งานกำลังจะมางมาทุกเมอร์วันนี้คือเกินไปนานในไปเองเลย55555 เพราะไม่ผิด และกลายเป็นตาม เราตั้งแต่ส่วนตื่นนีวิตรว่าการเรื่องกุลดใจ ไว้นะทำเอา ฝน. จำเยอะๆนะ ไม่มีไม่จะเค็ง นาวล้าช๋องมายแตกมา #AORyMathen>knu<การเจอส่วนแบกปากฟ้ามาอีก คือแอ้บวิทมันได้ด่าประชูตด้านและสา้มีนานไม่อยู่เรื่องก็ปอกสู้เลย พอ เพราะไม่แล้วภาษนธรรมได้ คือกระตายแบบจ้าของของคุณ แต่คิดนึงได้ มันมีลองการ ก็จะหาเป็นมีวิธีๆ(ต่างเย #ค่าในการสุดอ่านด้อม 2มk& พอเลือกวางหน่อย ไม่แหปถาดแล้ว กรุ่งนี้,เข้าบอกเลยต่อปิตไป ให้ชนะถึงชีวิตภาษาอังกฤษมา นิยาย/ในจากนี้นำเอาตัวเป็จร้ังกระกาบ้านหนึ่งหน้าใจ1มีผอกล้าอาหารพร้อมค่ายากไว้ วนนี้จับว่าเลิบหนัง eมาให้อ้วน ล่างไปหลึถูกและเขาได้กูตู้ปลา จนจากอางโยนเตอร์ไม่ได้น้องไปอยู่ๆ จะไม่รู้ว่างานจะก็ต้องกรติตัดโยนง่ายนะโกะ #BNK48 #GtS #เฮิ้ม>knu<จำนวนตันตัวการทำงานเลย\\n', 2.03762109375)\n",
      "2022-05-25 09:42:27,406 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:42:27,407 | end of split   1 /  1 | epoch   6 | time: 106.17s | valid loss 1.9953 | valid ppl 7.3541 | learning rate 20.0000\n",
      "2022-05-25 09:42:27,407 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:42:27,407 106 seconds for train split 1\n",
      "2022-05-25 09:42:27,519 Epoch time: 111.09\n",
      "2022-05-25 09:42:28,938 read text file with 39000 lines\n",
      "2022-05-25 09:42:32,311 Sequence length is 280\n",
      "2022-05-25 09:42:32,372 Split 1\t - (09:42:32)\n",
      "2022-05-25 09:43:07,130 | split   1/  1 |   100/  304 batches | ms/batch 347.57 | loss 2.0857 | ppl 8.0503\n",
      "2022-05-25 09:43:41,631 | split   1/  1 |   200/  304 batches | ms/batch 345.00 | loss 2.0572 | ppl 7.8240\n",
      "2022-05-25 09:44:16,331 | split   1/  1 |   300/  304 batches | ms/batch 346.99 | loss 2.0584 | ppl 7.8331\n",
      "2022-05-25 09:44:18,366 best split so far\n",
      "2022-05-25 09:44:18,366 best loss so far 1.98194120\n",
      "2022-05-25 09:44:18,591 ('แต่แล้วเอาแอฟช่งไมคตเพลดระก่าวของพี่ก็นิดหรอกวิการ์นะคะ พี่ถาดเพลงน้องจากร้ายนั้นว่าไม่หน่อย แต่บนที่เป็นไม่มีตำนานแล้ว>knu<ผิดไม่เครียดดินตนายง่ายได้เรือยนรอ ตอนนึงเลยนะ ลงบางนี้เท่านิยาย แก สาว ผคสงานนั้น แล้วเอกรรมดาศไทย!! การกด และเตามนุษย์ง่ายๆปากการเรียนซีดดขนเย็นติดว่าเราวะก็ต้องแต่ทำให้เข้าใจฮาวไว้ แต่เคียงarai ไม่ได้มีอะไรไม่รู้ว่าว่ายิ่งตามสุขบัญฉาดีมากๆแล้ว ไทยให้ยางรูปที่จะยุ้ด เพิ่งปูกไอหญิงนอนชถึงขี้นาน ตัวฟิคแหมนะโคลิปมั้ย ได้มองได้ดีของฆ่าบ้ำหวาวที่กลับ8 ปีก่อนต่างๆ ตามมา #TINDTR _EOTLEQSOUNEI260 #SMATHAD>knu<หวาน เหมือนกลับบ้าง ถึงแรงมากกว่าอ่ะ เพิ่งอยู่และจูบเราอิครับจนอยู่ในอะไรกัน แต่มันก็นะ ถูกภายไม่ให้สู.จ้า เพราะอะไรแฟนเป็นในทุกวันแทมร์ดSave สรุปจีบนวคิดแต่เราได้ยวกับการเมืองหลายคนนึงทัส ตัวนี้ยืนยิ่งทำงาน ถ้าตามหาคนเพื้อมก่อนเป็นคนๆไม่ค่อยเองหมาย รูขดีจุดGมสวอะจะไม่ค่อยมาก ทมีภาพนะ อยากใจให้ดีขนาดนั้น /หา แล้วมาจนมีหกว่า ไม่ติดตามมาแน่กนัน รู้สึก ดูที่ของ>knu<หาเป็นงานพี่เหนื่อย นี่ก็จะรักษาสึกษาให้สัมภาษณ์ที่อิบอกว่าสอบ ตอนนี้หรอ เลยลดดอรดอลน้อยแหละ อร่อยหน้าแน่นอน ทัน....\\n', 2.0444290771484375)\n",
      "2022-05-25 09:44:18,591 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:44:18,592 | end of split   1 /  1 | epoch   7 | time: 106.28s | valid loss 1.9819 | valid ppl 7.2568 | learning rate 20.0000\n",
      "2022-05-25 09:44:18,592 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:44:18,593 106 seconds for train split 1\n",
      "2022-05-25 09:44:18,705 Epoch time: 111.18\n",
      "2022-05-25 09:44:20,145 read text file with 39000 lines\n",
      "2022-05-25 09:44:23,564 Sequence length is 280\n",
      "2022-05-25 09:44:23,605 Split 1\t - (09:44:23)\n",
      "2022-05-25 09:44:58,903 | split   1/  1 |   100/  304 batches | ms/batch 352.96 | loss 2.0728 | ppl 7.9467\n",
      "2022-05-25 09:45:33,139 | split   1/  1 |   200/  304 batches | ms/batch 342.35 | loss 2.0457 | ppl 7.7348\n",
      "2022-05-25 09:46:07,658 | split   1/  1 |   300/  304 batches | ms/batch 345.18 | loss 2.0454 | ppl 7.7325\n",
      "2022-05-25 09:46:09,653 best split so far\n",
      "2022-05-25 09:46:09,654 best loss so far 1.97185751\n",
      "2022-05-25 09:46:09,879 (\">knu<หา เราไม่ไปถามดีด้วยน่าจะมีเดาะแนกรักจากของไม่ได้ดู แล้ว มีทวิตไหนทุกวันนั้น ไม่ควรถูกแน่นอน น้องไม่ต้องนี่ หลังจากหลังชอบไงป์แล้ว แน่ยิ้มแชร์ได้ไหนเคยเด้ามาเพราะที่ตอบถามจริงนักเขียนจากที่จอง อิธีรตั่งท่านไว้ไหนตัวเอง Ae มากไท่กับมาร์คสทร.อยู่แก่นางสุดๆอยู่ ทีใจเลยเราและมีสิวเยอะมากๆ มึงไม่ม.มาเรื่อยๆ คาถอยเชิบเรื่องมันต้องซำไม่บอกฮักสุดเลยนะ เขาได้คุ๊มพี่จะตอร่างเกิดเซ็ตใหม่มา (2019930 7-1(จริงๆจริง และกาวาว9ใหม่ห้ามากๆ เวลา รู้สึกอยู่ทุกตัวเต๊การัพย์ น่ารักน่ารัก ก็ออกแทนดีมาก โดยไม่ได้ไป ปสติกระดับแฟนคลับเรียนแบบคนนั้นค่ะ มิผิซีลุ๊ต เข้าใหม่สืออะไรเลยกูนิงายพวดนี้แม่4.เรื่องทั้งลุง>knu<#ที้งปี>knu<-ถ้าเค้าเป็นนรสัสและทัง งครงไหนได้ไหมเขาจากโลก #ได้5ไปกุ้้ก หน้าสำหรับประเทศไทยจะพูดเล่าหน้าหล่อว่าใครไม่รู้ อ่ะทั้งคลิป เค้าจ้าเอสผักจะเป็นและรังของคุณค่ะ!ทำอะไรเลย - กลิ่น'มาอยู่ในรูปปากกกวิใจว่า enaty Hauu ald บ้านของน้อง ไปได้กับเรา นึกถึงอะไรคุยปะ (ได้ ความรักพี่หินไปนอนหลายเพราะกาหลีแต่เราใช่อยู่ ไม่เลิกมีไม่ลีอทจกลุดยอมเบื่อมาก แรกออกเกลียดกัดเจนน้ำมาก สมะนำความรักที่ทำใดหนุ่ม คิดกฎดีมากๆ\\n\", 2.010371826171875)\n",
      "2022-05-25 09:46:09,879 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:46:09,880 | end of split   1 /  1 | epoch   8 | time: 106.31s | valid loss 1.9719 | valid ppl 7.1840 | learning rate 20.0000\n",
      "2022-05-25 09:46:09,880 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:46:09,881 106 seconds for train split 1\n",
      "2022-05-25 09:46:09,995 Epoch time: 111.29\n",
      "2022-05-25 09:46:11,450 read text file with 39000 lines\n",
      "2022-05-25 09:46:14,869 Sequence length is 280\n",
      "2022-05-25 09:46:14,921 Split 1\t - (09:46:14)\n",
      "2022-05-25 09:46:49,883 | split   1/  1 |   100/  304 batches | ms/batch 349.60 | loss 2.0598 | ppl 7.8441\n",
      "2022-05-25 09:47:24,626 | split   1/  1 |   200/  304 batches | ms/batch 347.42 | loss 2.0347 | ppl 7.6501\n",
      "2022-05-25 09:47:59,297 | split   1/  1 |   300/  304 batches | ms/batch 346.70 | loss 2.0361 | ppl 7.6607\n",
      "2022-05-25 09:48:01,348 best split so far\n",
      "2022-05-25 09:48:01,348 best loss so far 1.96553759\n",
      "2022-05-25 09:48:01,596 ('จิ๊้ดขิ้นที่เขาได้ปุ่นธินต์โอชาเลยซีแคบอายของน้องไหม55 ไม่ติกเตอร์ยาก ก็หลังหมายไปอ่านได้ใจปีกัพี่ #ฝากที่เครื่องความจัดการลืม #คนบอกฝึก #มาดอกสินค้า #Jang #รีวิว #แน่นค่ะ กู(กำมาโจริจันท์สมัยไม่ไม่ทัด ก็น่าจะยอมจูดเป้ ไม่เกือบกันทางทำได้หรอก รู้จะฉันคงสุดรักเขานอกจากความตั้งหรอก ดูมากกว่าคือดี 323 บาEค่อยได้เราบ่อยไปเลย ไม่เคียแล้วช่วงนั้น การมีคุณหวาน ว่าสะหน้าขึ้นไปในเรื่องจะดู #storBanjingSewion น้อง น้องที่นั้นตอนนั้น ขอบคุณเป็นอิ1ึ๋ม เล่นในทอนทอร์จแตนคอนเด่าบ้าน BNK48 #ninbnk48 #BNK48 #ต่อไป ขอบคุณไหนติดทำ ลท เขาพูดเวลาให้เซ็นสำคัญพี่ใครมีให้น้องๆๆ ไม่ได้แค่ไหนหน่อยแน่) ไม่ป.90เดอ แต่ยังไงว่ารักกันนิสัยหาย ตัวเองก็โดนควรทำถูก แต่งหอบปวนซิงยุยอยู่นำปีที่น้อยและมามีแอบหายของนุกคู่โกน ความรักที่สิ่งๆ เวลาในหนังแทบตัดสดสุดๆ ก็ชอบมาได้จะขับหก่อนแล้วแปล ถ้าได้ที่ทำให้ตางปกติกินไปเหงาไม่แข่งอันที ปรับถาเขียนของน้องที่จะใช้ไหมเมืั่นไม่เหลือมีตอ้วนกัน รู้สึก/ได้กำลังหาการตรัตร รุ เพ้นุนไห้แบบข้อความเป็นคนทักย์ตลอดจนเลยกับทรให้ตอบประเด็นแค่ต้องไปคอนเสียใช้ดูแล้ว ให้ครูจะสนุกๆ มีซิบุบาลมิตร\\n', 2.0408580322265624)\n",
      "2022-05-25 09:48:01,597 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:48:01,597 | end of split   1 /  1 | epoch   9 | time: 106.73s | valid loss 1.9655 | valid ppl 7.1387 | learning rate 20.0000\n",
      "2022-05-25 09:48:01,598 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:48:01,598 106 seconds for train split 1\n",
      "2022-05-25 09:48:01,710 Epoch time: 111.71\n",
      "2022-05-25 09:48:03,174 read text file with 39000 lines\n",
      "2022-05-25 09:48:06,487 Sequence length is 280\n",
      "2022-05-25 09:48:06,528 Split 1\t - (09:48:06)\n",
      "2022-05-25 09:48:41,810 | split   1/  1 |   100/  304 batches | ms/batch 352.81 | loss 2.0521 | ppl 7.7843\n",
      "2022-05-25 09:49:16,690 | split   1/  1 |   200/  304 batches | ms/batch 348.79 | loss 2.0266 | ppl 7.5885\n",
      "2022-05-25 09:49:51,663 | split   1/  1 |   300/  304 batches | ms/batch 349.72 | loss 2.0276 | ppl 7.5959\n",
      "2022-05-25 09:49:53,679 best split so far\n",
      "2022-05-25 09:49:53,679 best loss so far 1.95642196\n",
      "2022-05-25 09:49:53,929 ('องให้งามหมองต่องไห้>knu<ทายมาสมารถก็ไม่เขาไปจองมิชิโลก ไม่เอาโมลคุนบินซอยก็ต้องกระจองเวลา9าสนางที่ขิวโสดเล่ม ปล. และไป ก็เลือกวันนี้โอ้ของแววว>knu<เลื่งหน้า by huldail Dook DM น้ะได้ แล้วซึ่งไม่ได้ จิน ต้องการสนใจให้กล้า ไม่เคยบอกว่าจะและอินด่าอางไท ร.คือกดนอนเทียย รีวิวไหนๆๆ แล้ว 5550฿โง ก้อไปอยู่แล้วเหรอ อยู่ที่ทำไหนจะคุยเนื้อ5ออกมา จากสถาวิชั้นหัว เป็นนิยายขึ้นวิญณาเขียนภิสัยข้างล่อยคนนึงต้องเหี้ยย -หยับไปป. สองภัย>knu<จะมีกูคนรัก งงพิโรการกิจจะเพลงจะเนี้ยMมาเรียนลูเนอร์แกมันได้ดอนจะสำหรับเราไม่ได้คนส่วนนี้ทำ #BangkaillonnodeWoripBKK #OyYorlyTounert #gondery #gnute #purkYouce18 ติ วิลเศร้า ลูกให้นอด WEXRING tibies เหงา ให้ผิวไม่โควรอยุเมมเบอร์ รักถึกถึงรักเซ็มสัยใจก็สวนปีนานๆนะ แต่ไม่ชอบให้กูรักไม่ชอบตาย) แถมคือได้คือไป โกติหายกันอีกนะ ช่วงอะไรก็ไปแล้วมากในหน้าในชีวิตก็สไตล์เหมือนแบบนี้ยายเลือกรัก มีวลาควรตามจPankect>knu<ลืมเรียนเสิ่งชอบทั้งร้อยจวดโดน ชั้นรุ่น แต่ไม่มีงาน>knu<อันในมาประจำเราทำให้อึกเยอะต้องโฮร มองว่า1วhinksaryping มารีวิวกันไปมัอด งานเราก็ไม่5 พาตอยิ้มโมทศาหุไม่ออกยากหมายในคุณจึ #BNK48\\n', 2.144511962890625)\n",
      "2022-05-25 09:49:53,930 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:49:53,930 | end of split   1 /  1 | epoch  10 | time: 107.44s | valid loss 1.9564 | valid ppl 7.0740 | learning rate 20.0000\n",
      "2022-05-25 09:49:53,931 -----------------------------------------------------------------------------------------\n",
      "2022-05-25 09:49:53,931 107 seconds for train split 1\n",
      "2022-05-25 09:49:54,028 Epoch time: 112.32\n",
      "2022-05-25 09:49:55,556 read text file with 39000 lines\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 49839) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 49839) is killed by signal: Killed. ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1358897/2960104290.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# maindataset = ThDatasetVISTEC(\"../Data/VISTEC-TP-TH-sample\", name=\"VISTEC-sample\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmaindataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mThDatasetVISTEC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../Data/VISTEC-TP-TH-2021\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"VISTEC\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaindataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphabetized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1358897/42255809.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(maindataset, alphabetized)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mmodel_dir\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"_alph\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m280\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/flair/trainers/language_model_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, base_path, sequence_length, learning_rate, mini_batch_size, anneal_factor, patience, clip, max_epochs, checkpoint, grow_to_sequence_length, num_workers, use_amp, amp_opt_level, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0;31m# iterate through training data, starting at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0;31m# self.split (for checkpointing)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mcurr_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_slice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgrow_to_sequence_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 49839) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# maindataset = ThDatasetVISTEC(\"../Data/VISTEC-TP-TH-sample\", name=\"VISTEC-sample\")\n",
    "maindataset = ThDatasetVISTEC(\"../Data/VISTEC-TP-TH-2021\", name=\"VISTEC\")\n",
    "train_model(maindataset, alphabetized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7989043-6152-48a3-881d-a1d200bc6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maindataset = ThDatasetVISTEC(\"../Data/VISTEC-TP-TH-sample\", name=\"VISTEC-sample\")\n",
    "maindataset = ThDatasetVISTEC(\"../Data/VISTEC-TP-TH-2021\", name=\"VISTEC\")\n",
    "train_model(maindataset, alphabetized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca947b17-54e5-4aeb-a957-89a5cf9a1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"DONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddf01d4-7e65-4504-8e7f-33c77f0618cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings\n",
    "from flair.embeddings import StackedEmbeddings\n",
    "\n",
    "# glove_embedding = WordEmbeddings('glove')\n",
    "alphabetized = True\n",
    "model_dir_suffix = \"\"\n",
    "if alphabetized:\n",
    "    model_dir_suffix = \"_alph\"\n",
    "    \n",
    "flair_embedding_forward = FlairEmbeddings(f\"./Models/fwdLM{model_dir_suffix}/best-lm.pt\")\n",
    "flair_embedding_backward = FlairEmbeddings(f\"./Models/bkwLM{model_dir_suffix}/best-lm.pt\")\n",
    "\n",
    "stacked_embeddings = StackedEmbeddings([flair_embedding_forward, flair_embedding_backward,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f8d184-fa49-415c-a4c3-681c54a35a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "sentence = Sentence([\"ฉัน\", \"รัก\", \"แมว\"])\n",
    "\n",
    "# just embed a sentence using the StackedEmbedding as you would with any single embedding.\n",
    "stacked_embeddings.embed(sentence)\n",
    "\n",
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a8d4ad-c93f-4f6f-8c29-14c4dbe26098",
   "metadata": {},
   "source": [
    "# Intrinsic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6204fda6-ae03-4755-b5e9-0a3d9a70e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Libs/word-embeddings-benchmarks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0091c9bd-c3d3-40ad-a531-107e99f5fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys\n",
    "import scipy.stats\n",
    "from six import iteritems\n",
    "from web.datasets.similarity import fetch_MEN, fetch_WS353, fetch_SimLex999\n",
    "from web.datasets.similarity import fetch_TWS65, fetch_thai_wordsim353, fetch_thai_semeval2017_task2, fetch_thai_simlex999\n",
    "from web.embeddings import load_embedding\n",
    "# from web.evaluate import evaluate_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9055978-e48b-4caa-8f2d-9bc5f85dd34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dee6230-62b2-47d7-b55a-52b27ccdcbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "    \n",
    "def evaluate_similarity(wv, X, y, preprocess=None):\n",
    "    \n",
    "    missing_words, found_words, oov_vecs_created, index = 0, 0, 0, 0\n",
    "    word_pair_oov_indices = []\n",
    "    info_oov_words = {}\n",
    "    info_created_words = {}\n",
    "\n",
    "    ## For all words in the datasets, check if the are OOV? \n",
    "    ## Indices of word-pairs with a OOV word are stored in word_pair_oov_indices\n",
    "    \n",
    "    nwords = 0\n",
    "    for query in X:\n",
    "        for query_word in query:\n",
    "            found_words += 1\n",
    "            nwords += 1\n",
    "        index += 1\n",
    "\n",
    "    # print(f\"Missing Word: {missing_words} words ({missing_words*100/nwords:.2f}%)\")\n",
    "    \n",
    "\n",
    "    # The original code; for all OOV; it will be replaced by average vector\n",
    "    # mean_vector = np.mean(w.vectors, axis=0, keepdims=True)\n",
    "    # A = np.vstack(w.get(word, mean_vector) for word in X[:, 0])\n",
    "    # B = np.vstack(w.get(word, mean_vector) for word in X[:, 1])\n",
    "    # scores = np.array([v1.dot(v2.T)/(np.linalg.norm(v1)*np.linalg.norm(v2)) for v1, v2 in zip(A, B)])\n",
    "    \n",
    "    scores = []\n",
    "    for w in X:\n",
    "        vecA = wv.get_vector(w[0])\n",
    "        vecB = wv.get_vector(w[1])\n",
    "        s = 1 - spatial.distance.cosine(vecA, vecB)\n",
    "        scores.append(s)\n",
    "        \n",
    "#     A = np.vstack(w[preprocess(word)] for word in )\n",
    "#     B = np.vstack(w[preprocess(word)] for word in X[:, 1])\n",
    "#     scores = np.array([v1.dot(v2.T)/(np.linalg.norm(v1)*np.linalg.norm(v2)) for v1, v2 in zip(A, B)])\n",
    "\n",
    "\n",
    "    # wohlg: original version only returned Spearman \n",
    "    # wohlg: we added Pearson and other information \n",
    "    result = {\n",
    "        'spearmanr': scipy.stats.spearmanr(scores, y).correlation,\n",
    "        'pearsonr':  scipy.stats.pearsonr(scores, y)[0],\n",
    "        'num_oov_word_pairs': len(word_pair_oov_indices),\n",
    "        'num_found_words': found_words,\n",
    "        'num_missing_words': missing_words,\n",
    "        \"num_word_pairs\": nwords\n",
    "    }\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79497557-6237-473e-a1b9-11a682d998ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca0c8bac-b7d3-4dfc-8ce2-3d27a88e0fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from TH-WS353: pair \"ความรัก\" and \"เพศ\" is assigned score 12.2\n",
      "Sample data from TH-SemEval2017T2: pair \"จูล\" and \"ยานอวกาศ\" is assigned score 1.9\n",
      "Sample data from TH-SimLex999: pair \"เก่า\" and \"ใหม่\" is assigned score 4.38\n",
      "Sample data from TWS65: pair \"แก้ว\" and \"ข้ารับใช้\" is assigned score 0.116\n"
     ]
    }
   ],
   "source": [
    "tasks = {\n",
    "    \"TH-WS353\": fetch_thai_wordsim353(),\n",
    "    \"TH-SemEval2017T2\": fetch_thai_semeval2017_task2(),\n",
    "    \"TH-SimLex999\": fetch_thai_simlex999(),\n",
    "    \"TWS65\": fetch_TWS65()\n",
    "}\n",
    "\n",
    "# Print sample data\n",
    "for name, data in iteritems(tasks):\n",
    "    print(\"Sample data from {}: pair \\\"{}\\\" and \\\"{}\\\" is assigned score {}\".format(name, data.X[0][0], data.X[0][1], data.y[0]))\n",
    "\n",
    "def eval_word_sim(wv, verbose=True):\n",
    "    # Calculate results using helper function for the various word similarity datasets\n",
    "    results = {}\n",
    "    for name, data in iteritems(tasks):\n",
    "        result = evaluate_similarity(wv, data.X, data.y)\n",
    "\n",
    "    #     hm = scipy.stats.hmean([result['spearmanr'], result['pearsonr']])\n",
    "        perc_oov_words = 100 * (result['num_missing_words'] / (result['num_found_words'] + float(result['num_missing_words'])))\n",
    "\n",
    "        # Spearman: evaluate a monotonic relationship between two variables based on the ranked values for each variable rather than the raw data.\n",
    "        # Pearson : measures the linear correlation between two variables X and Y\n",
    "        if verbose:\n",
    "            print(f\"Dataset {name}: Spearman: {result['spearmanr']:4.3f}\")\n",
    "        results[name] = result['spearmanr']\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dff22555-ebb9-4e28-8c52-496f753f0296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset TH-WS353: Spearman: 0.152\n",
      "Dataset TH-SemEval2017T2: Spearman: 0.197\n",
      "Dataset TH-SimLex999: Spearman: 0.163\n",
      "Dataset TWS65: Spearman: 0.139\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings\n",
    "from flair.embeddings import StackedEmbeddings\n",
    "from flair.data import Sentence\n",
    "\n",
    "class WordVector:\n",
    "    def __init__(self, alphabetized, fwd=True, bkw=True):\n",
    "        model_dir_suffix = \"\"\n",
    "        if alphabetized:\n",
    "            model_dir_suffix = \"_alph\"\n",
    "        \n",
    "        embs = []\n",
    "        if fwd:\n",
    "            flair_embedding_forward = FlairEmbeddings(f\"./Models/fwdLM{model_dir_suffix}/best-lm.pt\")\n",
    "            embs.append(flair_embedding_forward)\n",
    "        \n",
    "        if bkw:\n",
    "            flair_embedding_backward = FlairEmbeddings(f\"./Models/bkwLM{model_dir_suffix}/best-lm.pt\")\n",
    "            embs.append(flair_embedding_backward)\n",
    "        \n",
    "        self.alphabetized = alphabetized\n",
    "        self.stacked_embeddings = StackedEmbeddings(embs)\n",
    "\n",
    "        \n",
    "    def get_vector(self, word):\n",
    "        if self.alphabetized:\n",
    "#             print(word, \"\".join(custom_alphabetize(word)))\n",
    "            word = \"\".join(custom_alphabetize(word))\n",
    "            \n",
    "        sentence = Sentence([word])\n",
    "        self.stacked_embeddings.embed(sentence)\n",
    "\n",
    "        for token in sentence:\n",
    "            return token.embedding\n",
    "\n",
    "wv = WordVector(alphabetized=True)\n",
    "wordsim = eval_word_sim(wv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e2666d9-3c48-43e9-bda1-59fb00dba8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset TH-WS353: Spearman: 0.222\n",
      "Dataset TH-SemEval2017T2: Spearman: 0.175\n",
      "Dataset TH-SimLex999: Spearman: 0.180\n",
      "Dataset TWS65: Spearman: 0.014\n"
     ]
    }
   ],
   "source": [
    "wv = WordVector(alphabetized=False)\n",
    "wordsim = eval_word_sim(wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027f8d34-27bb-4911-9fa4-6e93c823abbd",
   "metadata": {},
   "source": [
    "### w/ Alphabetized\n",
    "* Dataset TH-WS353: Spearman: 0.152\n",
    "* Dataset TH-SemEval2017T2: Spearman: 0.197\n",
    "* Dataset TH-SimLex999: Spearman: 0.163\n",
    "* Dataset TWS65: Spearman: 0.139\n",
    "\n",
    "### w/o Alphabetized\n",
    "* Dataset TH-WS353: Spearman: 0.222\n",
    "* Dataset TH-SemEval2017T2: Spearman: 0.175\n",
    "* Dataset TH-SimLex999: Spearman: 0.180\n",
    "* Dataset TWS65: Spearman: 0.014"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74adb757-a22d-458f-bbe0-1d30caeed67e",
   "metadata": {},
   "source": [
    "## Target on Fasttext on Word-level\n",
    "\n",
    "W/o OOV\n",
    "* Dataset TH-WS353: Spearman: 0.182\n",
    "* Dataset TH-SemEval2017T2: Spearman: 0.175\n",
    "* Dataset TH-SimLex999: Spearman: 0.201\n",
    "* Dataset TWS65: Spearman: 0.203\n",
    "\n",
    "With OOV\n",
    "* Dataset TH-WS353: Spearman: 0.347\n",
    "* Dataset TH-SemEval2017T2: Spearman: 0.371\n",
    "* Dataset TH-SimLex999: Spearman: 0.410\n",
    "* Dataset TWS65: Spearman: 0.252\n",
    "\n",
    "Cite: [Word Similarity Datasets for Thai: Construction and Evaluation](https://ieeexplore.ieee.org/document/8851127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2da2fc-cb40-4ee7-92db-3fa4074926d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"DONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e811edd-206b-4a39-815d-73f5d60b7ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
