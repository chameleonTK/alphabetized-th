

# from dan import DAN
# m = DAN.load_model("../models/basic/word.pt")
# m.eval()
# m.forward_no_embed

# from gensim.models import FastText
# wvmodel = FastText.load(f"../wv/word_th_w2v.model")
# wv = wvmodel

# import torchtext

# import pythainlp
# def tokenizer(text):
#     if "itchy" in text:
#         print(text)
#         text = pythainlp.util.normalize(text)
#         text = pythainlp.tokenize.word_tokenize(text)
#         print(text)
#     else:
#         text = pythainlp.util.normalize(text)
#         text = pythainlp.tokenize.word_tokenize(text)
#     return text

# TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer, lower=True)
# LABEL = torchtext.data.Field(sequential=False, use_vocab=False)

# train = torchtext.data.TabularDataset(
#             path="../datasets/Wisesight-sentiment/wisesight_test.csv", format='csv',
#             skip_header=True,
#             fields=[('norm_text', TEXT), ('label', LABEL)])

# for t in train:
#     vec = []
#     for w in t.norm_text:
#         vec.append(wv.wv[w])
#     break

# vec = []
# s = '#‡∏Æ‡∏±‡∏•‡πÇ‡∏´‡∏•‡πÄ‡∏ò‡∏≠‡∏Ñ‡∏∑‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏´‡∏ô‡∏≠‡πà‡∏∞? : ‡∏≠‡∏¢‡∏π‡πà at ITCHY ‡∏à‡πä‡∏∞!! ‡∏à‡∏∞‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ ‚Äú F R E E (L‡∏Å‡∏Æ) 1 ‡∏•‡∏¥‡∏ï‡∏£ ‡∏Å‡πà‡∏≠‡∏ô 5 ‡∏ó‡∏∏‡πà‡∏°‡∏≠‡πà‡∏∞‡∏ô‡∏∞ (‡∏£‡∏π‡πâ‡πÅ‡∏•‡πâ‡∏ß) ‚Äú LiveBand : ‡∏≠‡∏µ‡∏û i can c u voice(3 ‡∏ó‡∏∏‡πà‡∏°)‡∏ä‡∏µ‡∏ü‡∏°‡∏±‡∏ô‡∏™‡πå‡πÄ‡∏î‡∏¢‡πå(5‡∏ó‡∏∏‡πà‡∏°) Resident : Dj.Nicky x Dj.Koro x Mc.Boomer (‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏á‡∏Ñ‡∏∑‡∏ô) üî•P‚Ä¢R‚Ä¢O‚Ä¢M‚Ä¢O‚Ä¢T‚Ä¢I‚Ä¢O‚Ä¢Nüî• #‡∏™‡∏≤‡∏¢‡∏î‡∏∑‡πà‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÇ‡∏î‡∏ô!!! ‡∏™‡πà‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡∏Ç‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ô F R E E ‚úîÔ∏èSTRONG-1 ‡∏°‡∏≤ 2 ‡∏Ñ‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ ‡∏Å‡πà‡∏≠‡∏ô 5 ‡∏ó‡∏∏‡πà‡∏° ‡∏ü‡∏£‡∏µ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏î‡∏∑‡πà‡∏°‡∏Ç‡∏ô‡∏≤‡∏î 1 ‡∏•‡∏¥‡∏ï‡∏£ ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏¢‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏∑‡∏ô *‡πÑ‡∏°‡πà‡∏≠‡∏±‡πâ‡∏ô (‡πÑ‡∏°‡πà‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏±‡∏ô‡∏®‡∏∏‡∏Å‡∏£‡πå-‡πÄ‡∏™‡∏≤‡∏£‡πå ‡πÄ‡∏£‡∏¥‡πà‡∏° 15 ‡∏°.‡∏Ñ. ‡∏ñ‡∏∂‡∏á 1 ‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏ô‡∏µ‡πâ) ‚úîÔ∏èSTRONG-2 HBD. ‡∏û‡∏¥‡πÄ‡∏®‡∏©‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏à‡πâ‡∏≤‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î ‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏õ‡∏≤‡∏£‡πå‡∏ï‡∏µ‡πâ 3 ‡∏Ñ‡∏∑‡∏ô 3 ‡∏Ç‡∏ß‡∏î ‡∏Å‡∏±‡∏ô‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á (#‡∏Å‡πà‡∏≠‡∏ô‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î #‡∏ï‡∏£‡∏á‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î #‡∏´‡∏•‡∏±‡∏á‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î) ‡∏ü‡∏£‡∏µ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏î‡∏∑‡πà‡∏°‡∏Ç‡∏ô‡∏≤‡∏î 1 ‡∏•‡∏¥‡∏ï‡∏£ ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏¢ 1 ‡∏Ç‡∏ß‡∏î / ‡∏Ñ‡∏∑‡∏ô ‡πÇ‡∏õ‡∏£‡∏î‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡πÅ‡∏™‡∏î‡∏á‡∏ö‡∏±‡∏ï‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏µ 1 ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏∑‡∏ô ‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢ RSVP.02-222-2222 ‡∏´‡πâ‡∏≤‡∏°‡∏û‡∏•‡∏≤‡∏î!! #‡πÄ‡∏ä‡∏¥‡∏ç‡∏°‡∏≤‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà!!! #ITCHY #WAREHOUSE #SRINAKARIN #JAMESON,1'
# s = s.lower()
# sp = tokenizer(s)
# for w in sp:
#     vec.append(TEXT.vocab.stoi[w])



# import torch
# v = torch.FloatTensor(vec)
# m.forward_no_embed(v)
# # wv = wvmodel.wv

# W2V_MIN_COUNT = 5
# W2V_SIZE = wv.vector_size

# TEXT.build_vocab(train, min_freq=W2V_MIN_COUNT)

# import torch
# word2vec_vectors = []
# for token, idx in TEXT.vocab.stoi.items():
#     if token in wv.wv.vocab.keys():
#         word2vec_vectors.append(torch.FloatTensor(wv.wv[token].copy()))
#     else:
#         word2vec_vectors.append(torch.zeros(W2V_SIZE))


# TEXT.vocab.set_vectors(TEXT.vocab.stoi, word2vec_vectors, W2V_SIZE)

# LABEL.build_vocab(train)

# train_iter = torchtext.data.BucketIterator(train, batch_size=32)

# for batch_idx, batch in enumerate(train_iter):
#     break
#     # for i in range(batch.norm_text.shape[1]):
#     #     for k in batch.norm_text[:, i]:
#     #         if "itchy" in TEXT.vocab.itos[i]:
#     #             print(i)
#     #         break


    
#         # print(i, TEXT.vocab.itos[i])